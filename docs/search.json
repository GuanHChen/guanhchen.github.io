[
  {
    "objectID": "qtfiles/cv.html",
    "href": "qtfiles/cv.html",
    "title": "CV",
    "section": "",
    "text": "CV"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guan Chen",
    "section": "",
    "text": "Hello! I am currently a master’s student in the Social Data Analytics and Research program at the University of Texas at Dallas. I have a background as a scientist in bacterial genetics and membrane biology. My goal is to enhance my analytical abilities and pursue a career in data science.\nPlease feel free to look around my website and learn more about me!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Guan Chen",
    "section": "",
    "text": "Hello! I am currently a master’s student in the Social Data Analytics and Research program at the University of Texas at Dallas. I have a background as a scientist in bacterial genetics and membrane biology. My goal is to enhance my analytical abilities and pursue a career in data science.\nPlease feel free to look around my website and learn more about me!"
  },
  {
    "objectID": "6323/assignment8.html",
    "href": "6323/assignment8.html",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "require(ISLR)\nrequire(MASS)\nrequire(descr)\nattach(Smarket)\n\n## Linear Discriminant Analysis\nfreq(Direction)\n\n\n\n\n\n\n\n\nDirection \n      Frequency Percent\nDown        602   48.16\nUp          648   51.84\nTotal      1250  100.00\n\ntrain = Year&lt;2005\nlda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year&lt;2005)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = Year &lt; \n    2005)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit, col=\"dodgerblue\")\n\n\n\n\n\n\n\nSmarket.2005=subset(Smarket,Year==2005) # Creating subset with 2005 data for prediction\nlda.pred=predict(lda.fit,Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\nlda.class=lda.pred$class\nDirection.2005=Smarket$Direction[!train] \ntable(lda.class,Direction.2005) \n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\ndata.frame(lda.pred)[1:5,]\n\n     class posterior.Down posterior.Up         LD1\n999     Up      0.4901792    0.5098208  0.08293096\n1000    Up      0.4792185    0.5207815  0.59114102\n1001    Up      0.4668185    0.5331815  1.16723063\n1002    Up      0.4740011    0.5259989  0.83335022\n1003    Up      0.4927877    0.5072123 -0.03792892\n\ntable(lda.pred$class,Smarket.2005$Direction)\n\n      \n       Down  Up\n  Down   35  35\n  Up     76 106\n\nmean(lda.pred$class==Smarket.2005$Direction)\n\n[1] 0.5595238\n\n\nThe best subset selection model will have the smallest training rss. The stepwise selection models will have the smallest test rss. Between the forward and backward models, it will depend on the specific data being modeled.\n\nset.seed(123)\nx &lt;- rnorm(100)\neps &lt;- rnorm(100)\n\ny &lt;- 4 + 9 * x + 2 * x^2 + x^3 + eps\n\nplot(x)\n\n\n\n\n\n\n\nplot(y)\n\n\n\n\n\n\n\nrequire(leaps)\n\nbest_subset &lt;- regsubsets(y ~ poly(x, 10, raw = T), data = data.frame(y,x, nvmax = 10))\nbic &lt;- summary(best_subset)$bic\ncp &lt;- summary(best_subset)$cp\nadjr2 &lt;- summary(best_subset)$adjr2\n\nplot(bic, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"BIC Value\", \n     main = \"BIC Values for Different Models\")\n\n\n\n\n\n\n\nplot(cp, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"Cp Value\", \n     main = \"Cp Values for Different Models\")\n\n\n\n\n\n\n\nplot(adjr2, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"Adjusted R^2 Value\", \n     main = \"Adjusted R^2 Values for Different Models\")\n\n\n\n\n\n\n\nwhich.min(bic)\n\n[1] 3\n\nwhich.min(cp)\n\n[1] 3\n\nwhich.max(adjr2)\n\n[1] 7\n\ncoef(best_subset, id = 3)\n\n          (Intercept) poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 \n             3.970394              8.920446              1.908457 \npoly(x, 10, raw = T)3 \n             1.020436 \n\n\nThe best model is model 3 which has the lowest BIC value and Cp value. While model 7 has the maximum adjusted R squared value, the R squared plot begins to plateau at model 3, and I suspect overfitting to occur.\nThe coefficients for model 3 are 3.97, 8.92, 1.91, and 1.02 for the intercept, B1, B2, and B3 respectively.\n\nfor_subset &lt;- regsubsets(y ~ poly(x, 10, raw = T), data = data.frame(y,x, nvmax = 10), method = \"forward\")\n\nplot(summary(for_subset)$bic, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"BIC Value\", \n     main = \"BIC Values for Different Models\")\n\n\n\n\n\n\n\nplot(summary(for_subset)$cp, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"Cp Value\", \n     main = \"Cp Values for Different Models\")\n\n\n\n\n\n\n\nplot(summary(for_subset)$adjr2, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"Adjusted R^2 Value\", \n     main = \"Adjusted R^2 Values for Different Models\")\n\n\n\n\n\n\n\nwhich.min(summary(for_subset)$bic)\n\n[1] 3\n\nwhich.min(summary(for_subset)$cp)\n\n[1] 3\n\nwhich.max(summary(for_subset)$adjr2)\n\n[1] 4\n\ncoef(for_subset, id = 3)\n\n          (Intercept) poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 \n             3.970394              8.920446              1.908457 \npoly(x, 10, raw = T)3 \n             1.020436 \n\n\nThe best model is model 3 again with the lowest BIC and Cp values. Model 4 wins the best adjusted R squared value but model 3 has a similar adjusted R squared value according to the plot.\nThe coefficients for model 3 are 3.97, 8.92, 1.91, and 1.02 for the intercept, B1, B2, and B3 respectively.\n\nbac_subset &lt;- regsubsets(y ~ poly(x, 10, raw = T), data = data.frame(y,x, nvmax = 10), method = \"backward\")\n\nplot(summary(bac_subset)$bic, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"BIC Value\", \n     main = \"BIC Values for Different Models\")\n\n\n\n\n\n\n\nplot(summary(bac_subset)$cp, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"Cp Value\", \n     main = \"Cp Values for Different Models\")\n\n\n\n\n\n\n\nplot(summary(bac_subset)$adjr2, type = \"b\", pch = 16, col = \"blue\", \n     xlab = \"Model Number\", ylab = \"Adjusted R^2 Value\", \n     main = \"Adjusted R^2 Values for Different Models\")\n\n\n\n\n\n\n\nwhich.min(summary(bac_subset)$bic)\n\n[1] 4\n\nwhich.min(summary(bac_subset)$cp)\n\n[1] 4\n\nwhich.max(summary(bac_subset)$adjr2)\n\n[1] 4\n\ncoef(bac_subset, id = 3)\n\n          (Intercept) poly(x, 10, raw = T)1 poly(x, 10, raw = T)2 \n            3.9620068             9.8934015             1.9689642 \npoly(x, 10, raw = T)5 \n            0.1748705 \n\n\nUsing the backwards stepwise selection method, model 4 performs the best on all 3 metrics. The coefficients are 3.96, 9.89, 1.97, and 0.17 for the intercept, B1, B2, and B3 respectively."
  },
  {
    "objectID": "6323/assignment7.html",
    "href": "6323/assignment7.html",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "require(ISLR)\n\n# Check dataset Smarket\n?Smarket\nnames(Smarket)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\nsummary(Smarket)\n\n      Year           Lag1                Lag2                Lag3          \n Min.   :2001   Min.   :-4.922000   Min.   :-4.922000   Min.   :-4.922000  \n 1st Qu.:2002   1st Qu.:-0.639500   1st Qu.:-0.639500   1st Qu.:-0.640000  \n Median :2003   Median : 0.039000   Median : 0.039000   Median : 0.038500  \n Mean   :2003   Mean   : 0.003834   Mean   : 0.003919   Mean   : 0.001716  \n 3rd Qu.:2004   3rd Qu.: 0.596750   3rd Qu.: 0.596750   3rd Qu.: 0.596750  \n Max.   :2005   Max.   : 5.733000   Max.   : 5.733000   Max.   : 5.733000  \n      Lag4                Lag5              Volume           Today          \n Min.   :-4.922000   Min.   :-4.92200   Min.   :0.3561   Min.   :-4.922000  \n 1st Qu.:-0.640000   1st Qu.:-0.64000   1st Qu.:1.2574   1st Qu.:-0.639500  \n Median : 0.038500   Median : 0.03850   Median :1.4229   Median : 0.038500  \n Mean   : 0.001636   Mean   : 0.00561   Mean   :1.4783   Mean   : 0.003138  \n 3rd Qu.: 0.596750   3rd Qu.: 0.59700   3rd Qu.:1.6417   3rd Qu.: 0.596750  \n Max.   : 5.733000   Max.   : 5.73300   Max.   :3.1525   Max.   : 5.733000  \n Direction \n Down:602  \n Up  :648  \n           \n           \n           \n           \n\n# Create a dataframe for data browsing\nsm=Smarket\n\n# Bivariate Plot of inter-lag correlations\npairs(Smarket,col=Smarket$Direction,cex=.5, pch=20)\n\n\n\n\n\n\n\n# Logistic regression\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial)\nsummary(glm.fit)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = binomial, data = Smarket)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\nglm.probs=predict(glm.fit,type=\"response\") \nglm.probs[1:5]\n\n        1         2         3         4         5 \n0.5070841 0.4814679 0.4811388 0.5152224 0.5107812 \n\nglm.pred=ifelse(glm.probs&gt;0.5,\"Up\",\"Down\")\nattach(Smarket)\ntable(glm.pred,Direction)\n\n        Direction\nglm.pred Down  Up\n    Down  145 141\n    Up    457 507\n\nmean(glm.pred==Direction)\n\n[1] 0.5216\n\n# Make training and test set for prediction\ntrain = Year&lt;2005\nglm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs &gt;0.5,\"Up\",\"Down\")\nDirection.2005=Smarket$Direction[!train]\ntable(glm.pred,Direction.2005)\n\n        Direction.2005\nglm.pred Down Up\n    Down   77 97\n    Up     34 44\n\nmean(glm.pred==Direction.2005)\n\n[1] 0.4801587\n\n#Fit smaller model\nglm.fit=glm(Direction~Lag1+Lag2,\n            data=Smarket,family=binomial, subset=train)\nglm.probs=predict(glm.fit,newdata=Smarket[!train,],type=\"response\") \nglm.pred=ifelse(glm.probs &gt;0.5,\"Up\",\"Down\")\ntable(glm.pred,Direction.2005)\n\n        Direction.2005\nglm.pred Down  Up\n    Down   35  35\n    Up     76 106\n\nmean(glm.pred==Direction.2005)\n\n[1] 0.5595238\n\n# Check accuracy rate\n106/(76+106)\n\n[1] 0.5824176\n\n# Can you interpret the results?\n\n2a: Predictor variables are normally distributed, each predictor variable has the same variance, and the response variable must be categorical as LDA is used for classification problems.\n2b: LDA is used for multiclass classification whereas logistic regression is for binary classification. LDA also makes assumptions about the distribution of the data whereas logistic regression does not.\n2c: The receiving operator characteristic (ROC) evaluates the performance of a classifier model.\n2d: Sensitivity is the true positive rate. Specificity is the true negative rate. Generally, sensitivity is more important since there tends to be immediate and severe consequences for false positives (i.e. an innocent person who’s found guilty will go to jail) rather than false negatives. Neither type I or II errors are desirable, but high sensitivity should be prioritized.\n2e: Sensitivity is more important for prediction based on the chart.\n3:\n\nprediction_error &lt;- (252+23)/10000\ncat(\"Prediction error:\", formatC(prediction_error * 100, format = \"f\", digits = 2), \"%\\n\")\n\nPrediction error: 2.75 %"
  },
  {
    "objectID": "6323/lab04.html",
    "href": "6323/lab04.html",
    "title": "Knowledge Mining: Lab04 R programming (Unsupervised learning)",
    "section": "",
    "text": "Unsupervised learning is a class of machine learning algorithms to identify patterns or grouping structure in the data. Unlike supervised learning which relies on “supervised” information such as the dependent variable to guide modeling, unsupervised learning seeks to explore the structure and possible groupings of unlabeled data. This information will be useful to provide pre-processor for supervised learning.\nUnsupervised learning has no explicit dependent variable of Y for prediction. Instead, the goal is to discover interesting patterns about the measurements on \\((X_{1}), (X_{2}), . . . , (X_{p})\\) and identify any subgroups among the observations.\nGenerally, in this section, the two general methods are introduced: Principal components analysis and Clustering.\n\n\nPrincipal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] \nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) are the loadings of the first principal component; together, the loadings make up the principal component loading vector, \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.\n\n\n\n\n\nThe K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of \\((K)\\).\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level."
  },
  {
    "objectID": "6323/lab04.html#principal-component-analysis-pca",
    "href": "6323/lab04.html#principal-component-analysis-pca",
    "title": "Knowledge Mining: Lab04 R programming (Unsupervised learning)",
    "section": "",
    "text": "Principal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] \nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) are the loadings of the first principal component; together, the loadings make up the principal component loading vector, \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance."
  },
  {
    "objectID": "6323/lab04.html#clustering",
    "href": "6323/lab04.html#clustering",
    "title": "Knowledge Mining: Lab04 R programming (Unsupervised learning)",
    "section": "",
    "text": "The K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of \\((K)\\).\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level."
  },
  {
    "objectID": "6323/lab01.html",
    "href": "6323/lab01.html",
    "title": "Knowledge Mining: Lab01 R programming basics I",
    "section": "",
    "text": "\\[\ny = mx + b\n\\]\n\n\n\n\n[1] 1 3 2 5\n\n\n[1] 1 6 2\n\n\n\n\n\n\nlength(x)  # length() returns the number of indices in the vector\n\n[1] 3\n\nlength(y)\n\n[1] 3\n\n\n\n\n\n\nx+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!\n\n\n\n\n\n?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9922036\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)\n\n\n\n\n\nmean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768\n\n\n\n\n\n\nx=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "6323/lab01.html#create-object-using-the-assignment-operator--",
    "href": "6323/lab01.html#create-object-using-the-assignment-operator--",
    "title": "Knowledge Mining: Lab01 R programming basics I",
    "section": "",
    "text": "[1] 1 3 2 5\n\n\n[1] 1 6 2"
  },
  {
    "objectID": "6323/lab01.html#using-function",
    "href": "6323/lab01.html#using-function",
    "title": "Knowledge Mining: Lab01 R programming basics I",
    "section": "",
    "text": "length(x)  # length() returns the number of indices in the vector\n\n[1] 3\n\nlength(y)\n\n[1] 3"
  },
  {
    "objectID": "6323/lab01.html#using---operators",
    "href": "6323/lab01.html#using---operators",
    "title": "Knowledge Mining: Lab01 R programming basics I",
    "section": "",
    "text": "x+y\n\n[1]  2 10  5\n\nls() # List objects in the environment\n\n[1] \"x\" \"y\"\n\nrm(x,y) # Remove objects\nls()\n\ncharacter(0)\n\nrm(list=ls()) # Danger! What does this do?  Not recommended!"
  },
  {
    "objectID": "6323/lab01.html#matrix-operations",
    "href": "6323/lab01.html#matrix-operations",
    "title": "Knowledge Mining: Lab01 R programming basics I",
    "section": "",
    "text": "?matrix\nx=matrix(data=c(1,2,3,4), nrow=2, ncol=2) # Create a 2x2 matrix object\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx=matrix(c(1,2,3,4),2,2)\nmatrix(c(1,2,3,4),2,2,byrow=T) # What about byrow=F?\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n\nsqrt(x) # What does x look like?\n\n         [,1]     [,2]\n[1,] 1.000000 1.732051\n[2,] 1.414214 2.000000\n\nx\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\nx^2\n\n     [,1] [,2]\n[1,]    1    9\n[2,]    4   16\n\nx=rnorm(50) # Generate a vector of 50 numbers using the rnorm() function\n\ny=x+rnorm(50,mean=50,sd=.1) # What does rnorm(50,mean=50,sd=.1) generate?\n\ncor(x,y) # Correlation of x and y\n\n[1] 0.9922036\n\nset.seed(1303) # Set the seed for Random Number Generator (RNG) to generate values that are reproducible.\nrnorm(50)\n\n [1] -1.1439763145  1.3421293656  2.1853904757  0.5363925179  0.0631929665\n [6]  0.5022344825 -0.0004167247  0.5658198405 -0.5725226890 -1.1102250073\n[11] -0.0486871234 -0.6956562176  0.8289174803  0.2066528551 -0.2356745091\n[16] -0.5563104914 -0.3647543571  0.8623550343 -0.6307715354  0.3136021252\n[21] -0.9314953177  0.8238676185  0.5233707021  0.7069214120  0.4202043256\n[26] -0.2690521547 -1.5103172999 -0.6902124766 -0.1434719524 -1.0135274099\n[31]  1.5732737361  0.0127465055  0.8726470499  0.4220661905 -0.0188157917\n[36]  2.6157489689 -0.6931401748 -0.2663217810 -0.7206364412  1.3677342065\n[41]  0.2640073322  0.6321868074 -1.3306509858  0.0268888182  1.0406363208\n[46]  1.3120237985 -0.0300020767 -0.2500257125  0.0234144857  1.6598706557\n\nset.seed(3) # Try different seeds?\ny=rnorm(100)"
  },
  {
    "objectID": "6323/lab01.html#simple-descriptive-statistics-base",
    "href": "6323/lab01.html#simple-descriptive-statistics-base",
    "title": "Knowledge Mining: Lab01 R programming basics I",
    "section": "",
    "text": "mean(y)\n\n[1] 0.01103557\n\nvar(y)\n\n[1] 0.7328675\n\nsqrt(var(y))\n\n[1] 0.8560768\n\nsd(y)\n\n[1] 0.8560768"
  },
  {
    "objectID": "6323/lab01.html#visualization-using-r-graphics-without-packages",
    "href": "6323/lab01.html#visualization-using-r-graphics-without-packages",
    "title": "Knowledge Mining: Lab01 R programming basics I",
    "section": "",
    "text": "x=rnorm(100)\ny=rnorm(100)\nplot(x,y)\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"firebrick\") # Scatterplot for two numeric variables by default\n\n\n\n\n\n\n\nplot(x,y, pch=20, col = \"steelblue\",xlab=\"this is the x-axis\",ylab=\"this is the y-axis\",main=\"Plot of X vs Y\") # Add labels\n\n\n\n\n\n\n\npdf(\"Figure01.pdf\") # Save as pdf, add a path or it will be stored on the project directory\nplot(x,y,pch=20, col=\"forestgreen\") # Try different colors?\ndev.off() # Close the file using the dev.off function\n\nquartz_off_screen \n                2 \n\nx=seq(1,10) # Same as x=c(1:10)\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=1:10\nx\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nx=seq(-pi,pi,length=50)\ny=x"
  },
  {
    "objectID": "6323/lab03.html",
    "href": "6323/lab03.html",
    "title": "Knowledge Mining: Lab03 R programming (Exploratory Data Analysis)",
    "section": "",
    "text": "R Programming (EDA)\n\n## Creating a function: regplot\n## Combine the lm, plot and abline functions to create a regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y)\n  abline(fit,col=\"red\")\n}\n\n\nattach(ISLR::Carseats)\nregplot(Price,Sales)\n\n\n\n\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"red\")\n}  # \"...\" is called ellipsis, which is designed to take any number of named or unnamed arguments.\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"blue\",pch=20)\n\n\n\n\n\n\n\n\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot &lt;- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso &lt;- 0.05\n\n#Setup Axis\naxis_x &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y &lt;- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface &lt;- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length &lt;- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface &lt;- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot &lt;- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot &lt;- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot\n\n\n\n\n\n\n\nRegression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)\nsummary(petal_lm)\n\n\nCall:\nlm(formula = Petal.Length ~ 0 + Sepal.Length + Sepal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.70623 -0.51867 -0.08334  0.49844  1.93093 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nSepal.Length  1.56030    0.04557   34.24   &lt;2e-16 ***\nSepal.Width  -1.74570    0.08709  -20.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6869 on 148 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9726 \nF-statistic:  2663 on 2 and 148 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCalling TEDS_2016\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n\n\nFunction regplot\n\nregplot = function(x,y){\n  fit = lm(y~x)\n  plot(x,y)\n  abline(fit, col = \"red\")\n}\n\nregplot(TEDS_2016$age, TEDS_2016$Tondu)\n\n\n\n\n\n\n\nregplot(TEDS_2016$edu, TEDS_2016$Tondu)\n\n\n\n\n\n\n\nregplot(TEDS_2016$income, TEDS_2016$Tondu)\n\n\n\n\n\n\n\n\nThe regression plot does not appear to be very meaningful. This is due to the Tondu variable having 7 categories.\nSince the data is discrete, linear regression is not the most appropriate modeling method. I would opt to use logistic regression instead."
  },
  {
    "objectID": "6354/project.html",
    "href": "6354/project.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Project Proposal\nDownload PowerPoint File"
  },
  {
    "objectID": "6354/assignment1.html",
    "href": "6354/assignment1.html",
    "title": "Information Management (EPPS 6354)",
    "section": "",
    "text": "1: Name and describe three applications you have used that employed a database system to store and access persistent data. (e.g. airlines, online trade, banking, university system)\nThree applications the employ a database system include Reddit, Chase, and UTD Galaxy.\n\n\n2. Propose three applications in domain projects (e.g. criminology, economics, brain science, etc.) Be sure you include:\n\n\ni. Purpose\n\n\nii. Functions\n\n\niii. Simple interface design\nApplication 1: Crime Rate Predictor This application will use historical data to predict the likelihood of data in a specified geographic location. Police can then be optimally stationed around the city, in position to deter the highest likelihood of crime.\nApplication 2: Spending Predictor This application will be used to predict your future spending in a given month based on historical data, accounting for holidays and unforeseen accidents.\nApplication 3: Flu Tracker This application is aimed for use in the medical industry, keeping track of historical trends relating to the flu. This will predict the number of flu cases and allow hospitals to stock the appropriate amount of supplies such as vaccines.\n\n\n\n3. If data can be retrieved efficiently and effectively, why data mining is needed?\nData requires analysis (mining) in order to be useful. The raw data itself often does not reveal any substantial information.\n\n\n4. Why NoSQL systems emerged in the 2000s? Briefly contrast their features with traditional database systems.\nNoSQL allows developers to store unstructured data and have dynamic schemas. Conversely, traditional database systems use structured query language (SQL) and have determined schemas. NoSQL databases are non-relational while SQL databases are relational. Finally, NoSQL databases are horizontally scalable while SQL databases are vertically scalable.\n\n\n5. What are the things current database system cannot do?\nCurrent systems have scalability issues as well as query limitations.\n\n\n6. Describe at least three tables that might be used to store information in a social-network/social media system such as Twitter or Reddit.\nUser Table Stores information about specific users\nPost Table Stores information about specific post and its content\nRelationship Table Stores information about likes, dislikes, shares, etc from users"
  },
  {
    "objectID": "6354/assignment2.html",
    "href": "6354/assignment2.html",
    "title": "Information Management (EPPS 6354)",
    "section": "",
    "text": "1. What are the differences between relation schema, relation and instance? Give an example using the university database to illustrate.\nRelation schema is the overall design of the database. Relation refers to a single table in a relational database. Instance is a snapshot of the database in a moment of time.\nUsing the university database to illustrate, the overall database is the relation schema, the instructor table is a relation, and an instance of the database may be Fall 2019.\n\n\n2. Draw a schema diagram for the following bank database:\n\n\n\nBank Database\n\n\n\n\n3. Consider the above bank database. Assume that branch names (branch_name) and customer names (customer_name) uniquely identify branches and customers, but loans and accounts can be associated with more than one customer.\n\n\ni. What are the appropriate primary keys? (Underline each in diagram)\n\n\nii. Given your choice of primary keys, identify appropriate foreign keys."
  },
  {
    "objectID": "6323/lab02.html",
    "href": "6323/lab02.html",
    "title": "Knowledge Mining: Lab02 R programming basics II",
    "section": "",
    "text": "(Adapted from ISLR Chapter 3 Lab: Introduction to R)\n\n\n\nA=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4\n\n\n\n\n\n\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\"        \n\n\n\n\n\n\nAuto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9\n\n\n\n\n\n\n# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60 \n\n\n\n\n\n\nptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/vm/zdzk8kk927jgn_6691g9gfpc0000gn/T//RtmpWthcAE/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375 \n\n\n\n\n\n\n\n\n\n\n\n\n\nlm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)\n\n\n\n\n\nlm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n\n# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1\n\n\n\n\n\n\nsummary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6323/lab02.html#indexing-data-using",
    "href": "6323/lab02.html#indexing-data-using",
    "title": "Knowledge Mining: Lab02 R programming basics II",
    "section": "",
    "text": "A=matrix(1:16,4,4)\nA\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n[3,]    3    7   11   15\n[4,]    4    8   12   16\n\nA[2,3]\n\n[1] 10\n\nA[c(1,3),c(2,4)]\n\n     [,1] [,2]\n[1,]    5   13\n[2,]    7   15\n\nA[1:3,2:4]\n\n     [,1] [,2] [,3]\n[1,]    5    9   13\n[2,]    6   10   14\n[3,]    7   11   15\n\nA[1:2,]\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    5    9   13\n[2,]    2    6   10   14\n\nA[,1:2]\n\n     [,1] [,2]\n[1,]    1    5\n[2,]    2    6\n[3,]    3    7\n[4,]    4    8\n\nA[1,]\n\n[1]  1  5  9 13\n\nA[-c(1,3),] # What does -c() do?\n\n     [,1] [,2] [,3] [,4]\n[1,]    2    6   10   14\n[2,]    4    8   12   16\n\nA[-c(1,3),-c(1,3,4)]\n\n[1] 6 8\n\ndim(A) # Dimensions\n\n[1] 4 4"
  },
  {
    "objectID": "6323/lab02.html#loading-data-from-github-remote",
    "href": "6323/lab02.html#loading-data-from-github-remote",
    "title": "Knowledge Mining: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\")\nAuto=read.table(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.data\",header=T,na.strings=\"?\") \nAuto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\") # read csv file\n# Which function reads data faster?\n\n# Try using this simple method\n# time1 = proc.time()\n# Auto=read.csv(\"https://raw.githubusercontent.com/datageneration/knowledgemining/master/data/Auto.csv\",header=T,na.strings=\"?\")\n# proc.time()-time1\n\n# Check on data\ndim(Auto)\n\n[1] 397   9\n\nAuto[1:4,] # select rows\n\n  mpg cylinders displacement horsepower weight acceleration year origin\n1  18         8          307        130   3504         12.0   70      1\n2  15         8          350        165   3693         11.5   70      1\n3  18         8          318        150   3436         11.0   70      1\n4  16         8          304        150   3433         12.0   70      1\n                       name\n1 chevrolet chevelle malibu\n2         buick skylark 320\n3        plymouth satellite\n4             amc rebel sst\n\nAuto=na.omit(Auto)\ndim(Auto) # Notice the difference?\n\n[1] 397   9\n\nnames(Auto)\n\n[1] \"mpg\"          \"cylinders\"    \"displacement\" \"horsepower\"   \"weight\"      \n[6] \"acceleration\" \"year\"         \"origin\"       \"name\""
  },
  {
    "objectID": "6323/lab02.html#load-data-from-islr-website",
    "href": "6323/lab02.html#load-data-from-islr-website",
    "title": "Knowledge Mining: Lab02 R programming basics II",
    "section": "",
    "text": "Auto=read.table(\"https://www.statlearning.com/s/Auto.data\",header=T,na.strings=\"?\")\ndim(Auto)\n\n[1] 397   9"
  },
  {
    "objectID": "6323/lab02.html#additional-graphical-and-numerical-summaries",
    "href": "6323/lab02.html#additional-graphical-and-numerical-summaries",
    "title": "Knowledge Mining: Lab02 R programming basics II",
    "section": "",
    "text": "# plot(cylinders, mpg)\nplot(Auto$cylinders, Auto$mpg)\n\n\n\n\n\n\n\nattach(Auto)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\ncylinders=as.factor(cylinders)\nplot(cylinders, mpg)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\")\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T,horizontal=T)\n\n\n\n\n\n\n\nplot(cylinders, mpg, col=\"red\", varwidth=T, xlab=\"cylinders\", ylab=\"MPG\")\n\n\n\n\n\n\n\nhist(mpg)\n\n\n\n\n\n\n\nhist(mpg,col=2)\n\n\n\n\n\n\n\nhist(mpg,col=2,breaks=15)\n\n\n\n\n\n\n\n#pairs(Auto)\npairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)\n\n\n\n\n\n\n\nplot(horsepower,mpg)\n\n\n\n\n\n\n\n# identify(horsepower,mpg,name) # Interactive: point and click the dot to identify cases\nsummary(Auto)\n\n      mpg          cylinders      displacement     horsepower        weight    \n Min.   : 9.00   Min.   :3.000   Min.   : 68.0   Min.   : 46.0   Min.   :1613  \n 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   1st Qu.: 75.0   1st Qu.:2223  \n Median :23.00   Median :4.000   Median :146.0   Median : 93.5   Median :2800  \n Mean   :23.52   Mean   :5.458   Mean   :193.5   Mean   :104.5   Mean   :2970  \n 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   3rd Qu.:126.0   3rd Qu.:3609  \n Max.   :46.60   Max.   :8.000   Max.   :455.0   Max.   :230.0   Max.   :5140  \n                                                 NA's   :5                     \n  acceleration        year           origin          name          \n Min.   : 8.00   Min.   :70.00   Min.   :1.000   Length:397        \n 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   Class :character  \n Median :15.50   Median :76.00   Median :1.000   Mode  :character  \n Mean   :15.56   Mean   :75.99   Mean   :1.574                     \n 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000                     \n Max.   :24.80   Max.   :82.00   Max.   :3.000                     \n                                                                   \n\nsummary(mpg)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   9.00   17.50   23.00   23.52   29.00   46.60"
  },
  {
    "objectID": "6323/lab02.html#linear-regression",
    "href": "6323/lab02.html#linear-regression",
    "title": "Knowledge Mining: Lab02 R programming basics II",
    "section": "",
    "text": "ptbu=c(\"MASS\",\"ISLR\")\ninstall.packages(ptbu, repos='http://cran.us.r-project.org')\n\n\nThe downloaded binary packages are in\n    /var/folders/vm/zdzk8kk927jgn_6691g9gfpc0000gn/T//RtmpWthcAE/downloaded_packages\n\nlapply(ptbu, require, character.only = TRUE)\n\n[[1]]\n[1] TRUE\n\n[[2]]\n[1] TRUE\n\nlibrary(MASS)\nlibrary(ISLR)\n\n# Simple Linear Regression\n\n# fix(Boston)\nnames(Boston)\n\n [1] \"crim\"    \"zn\"      \"indus\"   \"chas\"    \"nox\"     \"rm\"      \"age\"    \n [8] \"dis\"     \"rad\"     \"tax\"     \"ptratio\" \"black\"   \"lstat\"   \"medv\"   \n\n# lm.fit=lm(medv~lstat)\nattach(Boston)\nlm.fit=lm(medv~lstat,data=Boston)\nattach(Boston)\nlm.fit=lm(medv~lstat)\nlm.fit\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nCoefficients:\n(Intercept)        lstat  \n      34.55        -0.95  \n\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.168  -3.990  -1.318   2.034  24.500 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 34.55384    0.56263   61.41   &lt;2e-16 ***\nlstat       -0.95005    0.03873  -24.53   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.216 on 504 degrees of freedom\nMultiple R-squared:  0.5441,    Adjusted R-squared:  0.5432 \nF-statistic: 601.6 on 1 and 504 DF,  p-value: &lt; 2.2e-16\n\nnames(lm.fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\ncoef(lm.fit)\n\n(Intercept)       lstat \n 34.5538409  -0.9500494 \n\nconfint(lm.fit)\n\n                2.5 %     97.5 %\n(Intercept) 33.448457 35.6592247\nlstat       -1.026148 -0.8739505\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"confidence\")\n\n       fit      lwr      upr\n1 29.80359 29.00741 30.59978\n2 25.05335 24.47413 25.63256\n3 20.30310 19.73159 20.87461\n\npredict(lm.fit,data.frame(lstat=(c(5,10,15))), interval=\"prediction\")\n\n       fit       lwr      upr\n1 29.80359 17.565675 42.04151\n2 25.05335 12.827626 37.27907\n3 20.30310  8.077742 32.52846\n\n# What is the differnce between \"conference\" and \"prediction\" difference?\n\nplot(lstat,medv)\nabline(lm.fit)\nabline(lm.fit,lwd=3)\nabline(lm.fit,lwd=3,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,col=\"red\")\n\n\n\n\n\n\n\nplot(lstat,medv,pch=16)\n\n\n\n\n\n\n\nplot(lstat,medv,pch=\"+\")\n\n\n\n\n\n\n\nplot(1:20,1:20,pch=1:20)\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nplot(lm.fit)\n\n\n\n\n\n\n\nplot(predict(lm.fit), residuals(lm.fit))\nplot(predict(lm.fit), rstudent(lm.fit))\nplot(hatvalues(lm.fit))\nwhich.max(hatvalues(lm.fit))\n\n375 \n375"
  },
  {
    "objectID": "6323/lab02.html#multiple-linear-regression",
    "href": "6323/lab02.html#multiple-linear-regression",
    "title": "Knowledge Mining: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit=lm(medv~lstat+age,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ lstat + age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.981  -3.978  -1.283   1.968  23.158 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 33.22276    0.73085  45.458  &lt; 2e-16 ***\nlstat       -1.03207    0.04819 -21.416  &lt; 2e-16 ***\nage          0.03454    0.01223   2.826  0.00491 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.173 on 503 degrees of freedom\nMultiple R-squared:  0.5513,    Adjusted R-squared:  0.5495 \nF-statistic:   309 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~.,data=Boston)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = medv ~ ., data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.595  -2.730  -0.518   1.777  26.199 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  3.646e+01  5.103e+00   7.144 3.28e-12 ***\ncrim        -1.080e-01  3.286e-02  -3.287 0.001087 ** \nzn           4.642e-02  1.373e-02   3.382 0.000778 ***\nindus        2.056e-02  6.150e-02   0.334 0.738288    \nchas         2.687e+00  8.616e-01   3.118 0.001925 ** \nnox         -1.777e+01  3.820e+00  -4.651 4.25e-06 ***\nrm           3.810e+00  4.179e-01   9.116  &lt; 2e-16 ***\nage          6.922e-04  1.321e-02   0.052 0.958229    \ndis         -1.476e+00  1.995e-01  -7.398 6.01e-13 ***\nrad          3.060e-01  6.635e-02   4.613 5.07e-06 ***\ntax         -1.233e-02  3.760e-03  -3.280 0.001112 ** \nptratio     -9.527e-01  1.308e-01  -7.283 1.31e-12 ***\nblack        9.312e-03  2.686e-03   3.467 0.000573 ***\nlstat       -5.248e-01  5.072e-02 -10.347  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.745 on 492 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7338 \nF-statistic: 108.1 on 13 and 492 DF,  p-value: &lt; 2.2e-16\n\nlibrary(car)\nvif(lm.fit)\n\n    crim       zn    indus     chas      nox       rm      age      dis \n1.792192 2.298758 3.991596 1.073995 4.393720 1.933744 3.100826 3.955945 \n     rad      tax  ptratio    black    lstat \n7.484496 9.008554 1.799084 1.348521 2.941491 \n\nlm.fit1=lm(medv~.-age,data=Boston)\nsummary(lm.fit1)\n\n\nCall:\nlm(formula = medv ~ . - age, data = Boston)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.6054  -2.7313  -0.5188   1.7601  26.2243 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  36.436927   5.080119   7.172 2.72e-12 ***\ncrim         -0.108006   0.032832  -3.290 0.001075 ** \nzn            0.046334   0.013613   3.404 0.000719 ***\nindus         0.020562   0.061433   0.335 0.737989    \nchas          2.689026   0.859598   3.128 0.001863 ** \nnox         -17.713540   3.679308  -4.814 1.97e-06 ***\nrm            3.814394   0.408480   9.338  &lt; 2e-16 ***\ndis          -1.478612   0.190611  -7.757 5.03e-14 ***\nrad           0.305786   0.066089   4.627 4.75e-06 ***\ntax          -0.012329   0.003755  -3.283 0.001099 ** \nptratio      -0.952211   0.130294  -7.308 1.10e-12 ***\nblack         0.009321   0.002678   3.481 0.000544 ***\nlstat        -0.523852   0.047625 -10.999  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.74 on 493 degrees of freedom\nMultiple R-squared:  0.7406,    Adjusted R-squared:  0.7343 \nF-statistic: 117.3 on 12 and 493 DF,  p-value: &lt; 2.2e-16\n\nlm.fit1=update(lm.fit, ~.-age)"
  },
  {
    "objectID": "6323/lab02.html#non-linear-transformations-of-the-predictors",
    "href": "6323/lab02.html#non-linear-transformations-of-the-predictors",
    "title": "Knowledge Mining: Lab02 R programming basics II",
    "section": "",
    "text": "lm.fit2=lm(medv~lstat+I(lstat^2))\nsummary(lm.fit2)\n\n\nCall:\nlm(formula = medv ~ lstat + I(lstat^2))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-15.2834  -3.8313  -0.5295   2.3095  25.4148 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 42.862007   0.872084   49.15   &lt;2e-16 ***\nlstat       -2.332821   0.123803  -18.84   &lt;2e-16 ***\nI(lstat^2)   0.043547   0.003745   11.63   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.524 on 503 degrees of freedom\nMultiple R-squared:  0.6407,    Adjusted R-squared:  0.6393 \nF-statistic: 448.5 on 2 and 503 DF,  p-value: &lt; 2.2e-16\n\nlm.fit=lm(medv~lstat)\nanova(lm.fit,lm.fit2)\n\nAnalysis of Variance Table\n\nModel 1: medv ~ lstat\nModel 2: medv ~ lstat + I(lstat^2)\n  Res.Df   RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    504 19472                                 \n2    503 15347  1    4125.1 135.2 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\npar(mfrow=c(2,2))\nplot(lm.fit2)\n\n\n\n\n\n\n\nlm.fit5=lm(medv~poly(lstat,5))\nsummary(lm.fit5)\n\n\nCall:\nlm(formula = medv ~ poly(lstat, 5))\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5433  -3.1039  -0.7052   2.0844  27.1153 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       22.5328     0.2318  97.197  &lt; 2e-16 ***\npoly(lstat, 5)1 -152.4595     5.2148 -29.236  &lt; 2e-16 ***\npoly(lstat, 5)2   64.2272     5.2148  12.316  &lt; 2e-16 ***\npoly(lstat, 5)3  -27.0511     5.2148  -5.187 3.10e-07 ***\npoly(lstat, 5)4   25.4517     5.2148   4.881 1.42e-06 ***\npoly(lstat, 5)5  -19.2524     5.2148  -3.692 0.000247 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.215 on 500 degrees of freedom\nMultiple R-squared:  0.6817,    Adjusted R-squared:  0.6785 \nF-statistic: 214.2 on 5 and 500 DF,  p-value: &lt; 2.2e-16\n\nsummary(lm(medv~log(rm),data=Boston))\n\n\nCall:\nlm(formula = medv ~ log(rm), data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-19.487  -2.875  -0.104   2.837  39.816 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -76.488      5.028  -15.21   &lt;2e-16 ***\nlog(rm)       54.055      2.739   19.73   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.915 on 504 degrees of freedom\nMultiple R-squared:  0.4358,    Adjusted R-squared:  0.4347 \nF-statistic: 389.3 on 1 and 504 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6323/lab02.html#qualitative-predictors",
    "href": "6323/lab02.html#qualitative-predictors",
    "title": "Knowledge Mining: Lab02 R programming basics II",
    "section": "",
    "text": "# fix(Carseats)\nnames(Carseats)\n\n [1] \"Sales\"       \"CompPrice\"   \"Income\"      \"Advertising\" \"Population\" \n [6] \"Price\"       \"ShelveLoc\"   \"Age\"         \"Education\"   \"Urban\"      \n[11] \"US\"         \n\nlm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)\nsummary(lm.fit)\n\n\nCall:\nlm(formula = Sales ~ . + Income:Advertising + Price:Age, data = Carseats)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9208 -0.7503  0.0177  0.6754  3.3413 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         6.5755654  1.0087470   6.519 2.22e-10 ***\nCompPrice           0.0929371  0.0041183  22.567  &lt; 2e-16 ***\nIncome              0.0108940  0.0026044   4.183 3.57e-05 ***\nAdvertising         0.0702462  0.0226091   3.107 0.002030 ** \nPopulation          0.0001592  0.0003679   0.433 0.665330    \nPrice              -0.1008064  0.0074399 -13.549  &lt; 2e-16 ***\nShelveLocGood       4.8486762  0.1528378  31.724  &lt; 2e-16 ***\nShelveLocMedium     1.9532620  0.1257682  15.531  &lt; 2e-16 ***\nAge                -0.0579466  0.0159506  -3.633 0.000318 ***\nEducation          -0.0208525  0.0196131  -1.063 0.288361    \nUrbanYes            0.1401597  0.1124019   1.247 0.213171    \nUSYes              -0.1575571  0.1489234  -1.058 0.290729    \nIncome:Advertising  0.0007510  0.0002784   2.698 0.007290 ** \nPrice:Age           0.0001068  0.0001333   0.801 0.423812    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.011 on 386 degrees of freedom\nMultiple R-squared:  0.8761,    Adjusted R-squared:  0.8719 \nF-statistic:   210 on 13 and 386 DF,  p-value: &lt; 2.2e-16\n\nattach(Carseats)\ncontrasts(ShelveLoc)\n\n       Good Medium\nBad       0      0\nGood      1      0\nMedium    0      1"
  },
  {
    "objectID": "6323/lab02.html#interaction-terms-including-interaction-and-single-effects",
    "href": "6323/lab02.html#interaction-terms-including-interaction-and-single-effects",
    "title": "Knowledge Mining: Lab02 R programming basics II",
    "section": "",
    "text": "summary(lm(medv~lstat*age,data=Boston))\n\n\nCall:\nlm(formula = medv ~ lstat * age, data = Boston)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.806  -4.045  -1.333   2.085  27.552 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 36.0885359  1.4698355  24.553  &lt; 2e-16 ***\nlstat       -1.3921168  0.1674555  -8.313 8.78e-16 ***\nage         -0.0007209  0.0198792  -0.036   0.9711    \nlstat:age    0.0041560  0.0018518   2.244   0.0252 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.149 on 502 degrees of freedom\nMultiple R-squared:  0.5557,    Adjusted R-squared:  0.5531 \nF-statistic: 209.3 on 3 and 502 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "6323/assignment1.html",
    "href": "6323/assignment1.html",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "Review of “To explain or to predict” by Galit Shmueli and “Statistical modeling: The two cultures” by Leo Breiman\nBoth papers talk about the same topic: the distinction between predictive and explanatory modeling. The authors would likely concur in that there is merit in either modeling technique depending on the circumstances. However, the authors adopt contrasting attitudes towards the use-cases for each modeling technique.\nLeo Breiman is a passionate advocate for predictive modeling, frequently citing its superiority in predictive accuracy as compared to more traditional statistical modeling methods. Furthermore, he criticizes statisticians for routinely attempting to employ explanatory modeling in situations where predictive modeling may perform better. Conversely, Galit Shmueli presents a more neutral tone and goes into deep detail regarding the different applications and advantages for each modeling technique respectively.\nThese differences in attitudes toward modeling techniques may be attributed to the differences in the authors background and experiences. Breiman has experience working in the industry whereas Shmueli has been dedicated to academia. Perhaps these attitudes are reflective of preferences within the environments they have worked in. I would hypothesize that predictive modeling yields more immediately actionable data which would benefit companies trying to make data driven choices while explanatory modeling is more conducive to making broad conclusions and associations. Ultimately, these modeling techniques have their respective advantages and disadvantages and statisticians should strive to employ them in the optimal conditions."
  },
  {
    "objectID": "6323/assignment2.html",
    "href": "6323/assignment2.html",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "library(haven)\nlibrary(tidyverse)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\n#Tondu vs Unification\nggplot(TEDS_2016, aes(Tondu)) +\n  geom_bar(aes(weight = Unification)) +\n  labs(x = \"Tondu\", y = \"Unification\")\n\n\n\n\n\n\n\n\nAt first glance, it is difficult to identify trends because a lot of the variables are binary (i.e. sex, party, whitecollar).\nMissing values can be handled by omitting the NA values.\n\n#Tondu vs female\nggplot(TEDS_2016, aes(female)) +\n  geom_bar(width = 0.5, aes(weight = Tondu)) +\n  labs(x = \"Sex\", y = \"Tondu\") \n\n\n\n\n\n\n\n#Tondu vs DPP\nggplot(TEDS_2016, aes(DPP)) +\n  geom_bar(width = 0.5, aes(weight = Tondu)) +\n  labs(x = \"DPP\", y = \"Tondu\")\n\n\n\n\n\n\n\n#Tondu vs age\nggplot(TEDS_2016, aes(age)) +\n  geom_bar(aes(weight = Tondu)) +\n  labs(x = \"Age\", y = \"Tondu\")\n\n\n\n\n\n\n\n#Tondu vs income\nggplot(TEDS_2016, aes(income)) +\n  geom_bar(aes(weight = Tondu)) +\n  labs(x = \"Income\", y = \"Tondu\")\n\n\n\n\n\n\n\n#Tondu vs edu\nggplot(TEDS_2016, aes(edu)) +\n  geom_bar(aes(weight = Tondu)) +\n  labs(x = \"Education\", y = \"Tondu\")\n\n\n\n\n\n\n\n#Tondu vs Taiwanese\nggplot(TEDS_2016, aes(Taiwanese)) +\n  geom_bar(width = 0.5, aes(weight = Tondu)) +\n  labs(x = \"Taiwanese\", y = \"Tondu\")\n\n\n\n\n\n\n\n#Tondu vs Econ_worse\nggplot(TEDS_2016, aes(Econ_worse)) +\n  geom_bar(width = 0.5, aes(weight = Tondu)) +\n  labs(x = \"Worse Economy\", y = \"Tondu\")\n\n\n\n\n\n\n\n#Tondu vs votetsai\nggplot(TEDS_2016, aes(votetsai)) +\n  geom_bar(width = 0.5, aes(weight = Tondu)) +\n  labs(x = \"Vote Tsai\", y = \"Tondu\")\n\n\n\n\n\n\n\ntable(TEDS_2016$Tondu)\n\n\n  1   2   3   4   5   6   9 \n 27 180 546 328 380 108 121 \n\nTEDS_2016$Tondu&lt;-as.numeric(TEDS_2016$Tondu,labels=c(\"Unification now”,\n“Status quo, unif. in future”, “Status quo, decide later\", \"Status quo\nforever\", \"Status quo, indep. in future\", \"Independence now”, “No response\"))\n\n#Tondu Frequency\nggplot(TEDS_2016, aes(Tondu)) +\n  geom_bar() +\n  scale_x_continuous(breaks=c(0:10)) +\n  labs(y = \"Frequency\")\n\n\n\n\n\n\n\n#install.packages(\"descr\")\n#descr::freq(TEDS_2016$Tondu)"
  },
  {
    "objectID": "6323/assignment6.html",
    "href": "6323/assignment6.html",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "require(ISLR)\nrequire(MASS)\nrequire(descr)\nattach(Smarket)\n\n## Linear Discriminant Analysis\nfreq(Direction)\n\n\n\n\n\n\n\n\nDirection \n      Frequency Percent\nDown        602   48.16\nUp          648   51.84\nTotal      1250  100.00\n\ntrain = Year&lt;2005\nlda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year&lt;2005)\nlda.fit\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = Smarket, subset = Year &lt; \n    2005)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\nplot(lda.fit, col=\"dodgerblue\")\n\n\n\n\n\n\n\nSmarket.2005=subset(Smarket,Year==2005) # Creating subset with 2005 data for prediction\nlda.pred=predict(lda.fit,Smarket.2005)\nnames(lda.pred)\n\n[1] \"class\"     \"posterior\" \"x\"        \n\nlda.class=lda.pred$class\nDirection.2005=Smarket$Direction[!train] \ntable(lda.class,Direction.2005) \n\n         Direction.2005\nlda.class Down  Up\n     Down   35  35\n     Up     76 106\n\ndata.frame(lda.pred)[1:5,]\n\n     class posterior.Down posterior.Up         LD1\n999     Up      0.4901792    0.5098208  0.08293096\n1000    Up      0.4792185    0.5207815  0.59114102\n1001    Up      0.4668185    0.5331815  1.16723063\n1002    Up      0.4740011    0.5259989  0.83335022\n1003    Up      0.4927877    0.5072123 -0.03792892\n\ntable(lda.pred$class,Smarket.2005$Direction)\n\n      \n       Down  Up\n  Down   35  35\n  Up     76 106\n\nmean(lda.pred$class==Smarket.2005$Direction)\n\n[1] 0.5595238\n\nlibrary(haven)\nTEDS_2016 &lt;- read_stata(\"https://github.com/datageneration/home/blob/master/DataProgramming/data/TEDS_2016.dta?raw=true\")\n\nglm.vt &lt;- glm(votetsai~female, data = TEDS_2016, family = binomial)\n\nsummary(glm.vt)\n\n\nCall:\nglm(formula = votetsai ~ female, family = binomial, data = TEDS_2016)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.54971    0.08245   6.667 2.61e-11 ***\nfemale      -0.06517    0.11644  -0.560    0.576    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1666.5  on 1260  degrees of freedom\nResidual deviance: 1666.2  on 1259  degrees of freedom\n  (429 observations deleted due to missingness)\nAIC: 1670.2\n\nNumber of Fisher Scoring iterations: 4\n\nplot(glm.vt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(ggplot2)\n\n\n\n\n\n\n\nggplot(TEDS_2016, aes(x = female, y = votetsai)) +\n  geom_bar(stat = \"identity\", fill = \"#1b98e0\") +\n  labs(title = \"Females Voting for Tsai\", x = \"Sex\", y = \"Votes\")\n\n\n\n\n\n\n\nglm.vt &lt;- glm(votetsai~female + KMT + DPP + age + edu + income, data = TEDS_2016, family = binomial)\n\nsummary(glm.vt)\n\n\nCall:\nglm(formula = votetsai ~ female + KMT + DPP + age + edu + income, \n    family = binomial, data = TEDS_2016)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.618640   0.592084   2.734  0.00626 ** \nfemale       0.047406   0.177403   0.267  0.78930    \nKMT         -3.156273   0.250360 -12.607  &lt; 2e-16 ***\nDPP          2.888943   0.267968  10.781  &lt; 2e-16 ***\nage         -0.011808   0.007164  -1.648  0.09931 .  \nedu         -0.184604   0.083102  -2.221  0.02632 *  \nincome       0.013727   0.034382   0.399  0.68971    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1661.76  on 1256  degrees of freedom\nResidual deviance:  836.15  on 1250  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 850.15\n\nNumber of Fisher Scoring iterations: 6\n\nplot(glm.vt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nglm.vt &lt;- glm(votetsai~female + KMT + DPP + age + edu + income + Independence + Econ_worse + Govt_dont_care + Minnan_father + Mainland_father + Taiwanese, data = TEDS_2016, family = binomial)\n\nsummary(glm.vt)\n\n\nCall:\nglm(formula = votetsai ~ female + KMT + DPP + age + edu + income + \n    Independence + Econ_worse + Govt_dont_care + Minnan_father + \n    Mainland_father + Taiwanese, family = binomial, data = TEDS_2016)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.015976   0.679780  -0.024  0.98125    \nfemale          -0.097996   0.189840  -0.516  0.60571    \nKMT             -2.922246   0.259333 -11.268  &lt; 2e-16 ***\nDPP              2.468855   0.275350   8.966  &lt; 2e-16 ***\nage              0.003287   0.007884   0.417  0.67672    \nedu             -0.092110   0.090119  -1.022  0.30674    \nincome           0.021771   0.036406   0.598  0.54984    \nIndependence     1.020953   0.251776   4.055 5.01e-05 ***\nEcon_worse       0.310462   0.189100   1.642  0.10063    \nGovt_dont_care  -0.014295   0.188765  -0.076  0.93964    \nMinnan_father   -0.247650   0.253921  -0.975  0.32941    \nMainland_father -1.089332   0.396822  -2.745  0.00605 ** \nTaiwanese        0.909019   0.198930   4.570 4.89e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1661.76  on 1256  degrees of freedom\nResidual deviance:  767.13  on 1244  degrees of freedom\n  (433 observations deleted due to missingness)\nAIC: 793.13\n\nNumber of Fisher Scoring iterations: 6\n\nplot(glm.vt)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFemale voters are slightly less likely to vote for President Tsai, though whether or not a voter is female is a statistically insignificant predictor.\nThe best predictors for whether or not someone will vote for President Tsai include KMT, DPP, Independence, Mainland_father, and Taiwanese."
  },
  {
    "objectID": "6323/assignment4.html",
    "href": "6323/assignment4.html",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "library(datasets)\nlibrary(ISLR)\narrest = USArrests\nstates = row.names(USArrests)\nnames(USArrests)\n\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"    \n\napply(USArrests, 2, mean)\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\napply(USArrests, 2, var)\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\npr.out=prcomp(USArrests, scale=TRUE)\nnames(pr.out)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\npr.out$center\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\npr.out$scale\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\npr.out$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\ndim(pr.out$x)\n\n[1] 50  4\n\npr.out$rotation=-pr.out$rotation\npr.out$x=-pr.out$x\nbiplot(pr.out, scale=0)\n\n\n\n\n\n\n\npr.out$sdev\n\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\npr.var=pr.out$sdev^2\npr.var\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\npve=pr.var/sum(pr.var)\npve\n\n[1] 0.62006039 0.24744129 0.08914080 0.04335752\n\nplot(pve, xlab=\"Principal Component\", ylab=\"Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\n\n\n\n\n\n\nplot(cumsum(pve), xlab=\"Principal Component\", ylab=\"Cumulative Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\nlibrary(factoextra)\n\n\n\n\n\n\n\nfviz(pr.out, \"ind\", geom = \"auto\", mean.point = TRUE, font.family = \"Georgia\")\n\n\n\n\n\n\n\nfviz_pca_biplot(pr.out, font.family = \"Georgia\", col.var=\"firebrick1\")\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\ncomputers = read.csv(\"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/computers.csv\") \n\n# Only retain two variables for illustration\nrescaled_comp &lt;- computers[4:5] %&gt;%\n  mutate(hd_scal = scale(hd),\n         ram_scal = scale(ram)) %&gt;%\n  select(c(hd_scal, ram_scal))\n        \nggplot(data = rescaled_comp, aes(x = hd_scal, y = ram_scal)) +\n  geom_point(pch=20, col = \"blue\") + theme_bw() +\n  labs(x = \"Hard drive size (Scaled)\", y =\"RAM size (Scaled)\" ) +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n\n\n\n\nlibrary(animation)\nset.seed(2345)\nlibrary(animation)\n\n# Animate the K-mean clustering process, cluster no. = 4\nkmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n\n\n\n\n\n\nggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n\n\n\n\n\n\nset.seed(20)\nirisCluster &lt;- kmeans(iris[, 3:4], 3, nstart = 20)\nirisCluster\n\nK-means clustering with 3 clusters of sizes 50, 48, 52\n\nCluster means:\n  Petal.Length Petal.Width\n1     1.462000    0.246000\n2     5.595833    2.037500\n3     4.269231    1.342308\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [75] 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 3 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\nWithin cluster sum of squares by cluster:\n[1]  2.02200 16.29167 13.05769\n (between_SS / total_SS =  94.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nclass(irisCluster$cluster)\n\n[1] \"integer\"\n\ntable(irisCluster$cluster, iris$Species)\n\n   \n    setosa versicolor virginica\n  1     50          0         0\n  2      0          2        46\n  3      0         48         4\n\nirisCluster$cluster &lt;- as.factor(irisCluster$cluster)\nggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme_bw()\n\n\n\n\n\n\n\nactual = ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nkmc = ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\", \"darkblue\", \"forestgreen\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nlibrary(grid)\nlibrary(gridExtra)\ngrid.arrange(arrangeGrob(actual, kmc, ncol=2, widths=c(1,1)), nrow=1)\n\n\n\n\n\n\n\nlibrary(readr)\nwine &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/gentlemachinelearning/master/data/wine.csv\")\n\n\n## Choose and scale variables\nwine_subset &lt;- scale(wine[ , c(2:4)])\n\n## Create cluster using k-means, k = 3, with 25 initial configurations\nwine_cluster &lt;- kmeans(wine_subset, centers = 3,\n                       iter.max = 10,\n                       nstart = 25)\nwine_cluster\n\nK-means clustering with 3 clusters of sizes 48, 60, 70\n\nCluster means:\n     Alcohol      Malic        Ash\n1  0.1470536  1.3907328  0.2534220\n2  0.8914655 -0.4522073  0.5406223\n3 -0.8649501 -0.5660390 -0.6371656\n\nClustering vector:\n  [1] 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2\n [38] 2 3 1 2 1 2 1 3 1 1 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 2 3 3 2 2 2\n [75] 3 3 3 3 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 1 3 3 3 3 3 1 3 3 2 1 1 1 3 3 3 3 1 3 1 3 1 3 3 1 1 1 1 1 2 1 1 1 1 1 1\n[149] 1 1 1 1 2 1 3 1 1 1 2 2 1 1 1 1 2 1 1 1 2 1 3 3 2 1 1 1 2 1\n\nWithin cluster sum of squares by cluster:\n[1]  73.71460  67.98619 111.63512\n (between_SS / total_SS =  52.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nwssplot &lt;- function(data, nc=15, seed=1234){\n  wss &lt;- (nrow(data)-1)*sum(apply(data,2,var))\n  for (i in 2:nc){\n    set.seed(seed)\n    wss[i] &lt;- sum(kmeans(data, centers=i)$withinss)}\n  plot(1:nc, wss, type=\"b\", xlab=\"Number of Clusters\",\n       ylab=\"Within groups sum of squares\")\n}\n\n# plotting values for each cluster starting from 1 to 9\nwssplot(wine_subset, nc = 9)\n\n\n\n\n\n\n\nwine_cluster$cluster = as.factor(wine_cluster$cluster)\npairs(wine[2:4],\n      col = c(\"firebrick1\", \"darkblue\", \"forestgreen\")[wine_cluster$cluster],\n      pch = c(15:17)[wine_cluster$cluster],\n      main = \"K-Means Clusters: Wine data\")\n\n\n\n\n\n\n\ntable(wine_cluster$cluster)\n\n\n 1  2  3 \n48 60 70 \n\nlibrary(factoextra)\nfviz_nbclust(wine_subset, kmeans, method = \"wss\")\n\n\n\n\n\n\n\nwine.km &lt;- eclust(wine_subset, \"kmeans\", nboot = 2)\n\n\n\n\n\n\n\nwine.km\n\nK-means clustering with 3 clusters of sizes 60, 70, 48\n\nCluster means:\n     Alcohol      Malic        Ash\n1  0.8914655 -0.4522073  0.5406223\n2 -0.8649501 -0.5660390 -0.6371656\n3  0.1470536  1.3907328  0.2534220\n\nClustering vector:\n  [1] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n [38] 1 2 3 1 3 1 3 2 3 3 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 2 2 1 1 1\n [75] 2 2 2 2 2 3 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[112] 2 3 2 2 2 2 2 3 2 2 1 3 3 3 2 2 2 2 3 2 3 2 3 2 2 3 3 3 3 3 1 3 3 3 3 3 3\n[149] 3 3 3 3 1 3 2 3 3 3 1 1 3 3 3 3 1 3 3 3 1 3 2 2 1 3 3 3 1 3\n\nWithin cluster sum of squares by cluster:\n[1]  67.98619 111.63512  73.71460\n (between_SS / total_SS =  52.3 %)\n\nAvailable components:\n\n [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"       \"clust_plot\"  \n[11] \"silinfo\"      \"nbclust\"      \"data\"         \"gap_stat\"    \n\nwine.km$nbclust\n\n[1] 3\n\nfviz_nbclust(wine_subset, kmeans, method = \"gap_stat\")\n\n\n\n\n\n\n\nfviz_silhouette(wine.km)\n\n  cluster size ave.sil.width\n1       1   60          0.44\n2       2   70          0.33\n3       3   48          0.30\n\n\n\n\n\n\n\n\nfviz_cluster(wine_cluster, data = wine_subset) + \n  theme_bw() +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n\n\n\n\nfviz_cluster(wine_cluster, data = wine_subset, ellipse.type = \"norm\") + \n  theme_bw() +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n\n\n\n\n\n\n\n\n\narrest.hc &lt;- USArrests %&gt;%\n  scale() %&gt;%                    # Scale all variables\n  dist(method = \"euclidean\") %&gt;% # Euclidean distance for dissimilarity \n  hclust(method = \"ward.D2\")     # Compute hierarchical clustering\n\n# Generate dendrogram using factoextra package\nfviz_dend(arrest.hc, k = 4, # Four groups\n          cex = 0.5, \n          k_colors = c(\"firebrick1\",\"forestgreen\",\"blue\", \"purple\"),\n          color_labels_by_k = TRUE, # color labels by groups\n          rect = TRUE, # Add rectangle (cluster) around groups,\n          main = \"Cluster Dendrogram: USA Arrest data\"\n) + theme(text = element_text(family=\"Georgia\")) \n\n\n\n\n\n\n\n\nReferences\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013 An introduction to statistical learning. Vol. 112. New York: Springer."
  },
  {
    "objectID": "6323/assignment4.html#principal-components-analysis",
    "href": "6323/assignment4.html#principal-components-analysis",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "library(datasets)\nlibrary(ISLR)\narrest = USArrests\nstates = row.names(USArrests)\nnames(USArrests)\n\n[1] \"Murder\"   \"Assault\"  \"UrbanPop\" \"Rape\"    \n\napply(USArrests, 2, mean)\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\napply(USArrests, 2, var)\n\n    Murder    Assault   UrbanPop       Rape \n  18.97047 6945.16571  209.51878   87.72916 \n\npr.out=prcomp(USArrests, scale=TRUE)\nnames(pr.out)\n\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\npr.out$center\n\n  Murder  Assault UrbanPop     Rape \n   7.788  170.760   65.540   21.232 \n\npr.out$scale\n\n   Murder   Assault  UrbanPop      Rape \n 4.355510 83.337661 14.474763  9.366385 \n\npr.out$rotation\n\n                PC1        PC2        PC3         PC4\nMurder   -0.5358995 -0.4181809  0.3412327  0.64922780\nAssault  -0.5831836 -0.1879856  0.2681484 -0.74340748\nUrbanPop -0.2781909  0.8728062  0.3780158  0.13387773\nRape     -0.5434321  0.1673186 -0.8177779  0.08902432\n\ndim(pr.out$x)\n\n[1] 50  4\n\npr.out$rotation=-pr.out$rotation\npr.out$x=-pr.out$x\nbiplot(pr.out, scale=0)\n\n\n\n\n\n\n\npr.out$sdev\n\n[1] 1.5748783 0.9948694 0.5971291 0.4164494\n\npr.var=pr.out$sdev^2\npr.var\n\n[1] 2.4802416 0.9897652 0.3565632 0.1734301\n\npve=pr.var/sum(pr.var)\npve\n\n[1] 0.62006039 0.24744129 0.08914080 0.04335752\n\nplot(pve, xlab=\"Principal Component\", ylab=\"Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\n\n\n\n\n\n\nplot(cumsum(pve), xlab=\"Principal Component\", ylab=\"Cumulative Proportion of Variance Explained\", ylim=c(0,1),type='b')\n\nlibrary(factoextra)\n\n\n\n\n\n\n\nfviz(pr.out, \"ind\", geom = \"auto\", mean.point = TRUE, font.family = \"Georgia\")\n\n\n\n\n\n\n\nfviz_pca_biplot(pr.out, font.family = \"Georgia\", col.var=\"firebrick1\")"
  },
  {
    "objectID": "6323/assignment4.html#k-means-clustering",
    "href": "6323/assignment4.html#k-means-clustering",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "library(dplyr)\nlibrary(ggplot2)\nlibrary(RColorBrewer)\n\ncomputers = read.csv(\"https://raw.githubusercontent.com/guru99-edu/R-Programming/master/computers.csv\") \n\n# Only retain two variables for illustration\nrescaled_comp &lt;- computers[4:5] %&gt;%\n  mutate(hd_scal = scale(hd),\n         ram_scal = scale(ram)) %&gt;%\n  select(c(hd_scal, ram_scal))\n        \nggplot(data = rescaled_comp, aes(x = hd_scal, y = ram_scal)) +\n  geom_point(pch=20, col = \"blue\") + theme_bw() +\n  labs(x = \"Hard drive size (Scaled)\", y =\"RAM size (Scaled)\" ) +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n\n\n\n\nlibrary(animation)\nset.seed(2345)\nlibrary(animation)\n\n# Animate the K-mean clustering process, cluster no. = 4\nkmeans.ani(rescaled_comp[1:2], centers = 4, pch = 15:18, col = 1:4) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(iris, aes(Petal.Length, Petal.Width)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n\n\n\n\n\n\nggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\"))\n\n\n\n\n\n\n\nset.seed(20)\nirisCluster &lt;- kmeans(iris[, 3:4], 3, nstart = 20)\nirisCluster\n\nK-means clustering with 3 clusters of sizes 50, 48, 52\n\nCluster means:\n  Petal.Length Petal.Width\n1     1.462000    0.246000\n2     5.595833    2.037500\n3     4.269231    1.342308\n\nClustering vector:\n  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [75] 3 3 3 2 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 3 2 2 2 2\n[112] 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2\n[149] 2 2\n\nWithin cluster sum of squares by cluster:\n[1]  2.02200 16.29167 13.05769\n (between_SS / total_SS =  94.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nclass(irisCluster$cluster)\n\n[1] \"integer\"\n\ntable(irisCluster$cluster, iris$Species)\n\n   \n    setosa versicolor virginica\n  1     50          0         0\n  2      0          2        46\n  3      0         48         4\n\nirisCluster$cluster &lt;- as.factor(irisCluster$cluster)\nggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme_bw()\n\n\n\n\n\n\n\nactual = ggplot(iris, aes(Petal.Length, Petal.Width, color = Species)) + geom_point() + \n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\",\"forestgreen\",\"darkblue\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nkmc = ggplot(iris, aes(Petal.Length, Petal.Width, color = irisCluster$cluster)) + geom_point() +\n  theme_bw() +\n  scale_color_manual(values=c(\"firebrick1\", \"darkblue\", \"forestgreen\")) +\n  theme(legend.position=\"bottom\") +\n  theme(text = element_text(family=\"Georgia\")) \nlibrary(grid)\nlibrary(gridExtra)\ngrid.arrange(arrangeGrob(actual, kmc, ncol=2, widths=c(1,1)), nrow=1)\n\n\n\n\n\n\n\nlibrary(readr)\nwine &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/gentlemachinelearning/master/data/wine.csv\")\n\n\n## Choose and scale variables\nwine_subset &lt;- scale(wine[ , c(2:4)])\n\n## Create cluster using k-means, k = 3, with 25 initial configurations\nwine_cluster &lt;- kmeans(wine_subset, centers = 3,\n                       iter.max = 10,\n                       nstart = 25)\nwine_cluster\n\nK-means clustering with 3 clusters of sizes 48, 60, 70\n\nCluster means:\n     Alcohol      Malic        Ash\n1  0.1470536  1.3907328  0.2534220\n2  0.8914655 -0.4522073  0.5406223\n3 -0.8649501 -0.5660390 -0.6371656\n\nClustering vector:\n  [1] 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 1 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2\n [38] 2 3 1 2 1 2 1 3 1 1 2 2 2 3 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 2 3 3 2 2 2\n [75] 3 3 3 3 3 1 3 3 3 1 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n[112] 3 1 3 3 3 3 3 1 3 3 2 1 1 1 3 3 3 3 1 3 1 3 1 3 3 1 1 1 1 1 2 1 1 1 1 1 1\n[149] 1 1 1 1 2 1 3 1 1 1 2 2 1 1 1 1 2 1 1 1 2 1 3 3 2 1 1 1 2 1\n\nWithin cluster sum of squares by cluster:\n[1]  73.71460  67.98619 111.63512\n (between_SS / total_SS =  52.3 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\nwssplot &lt;- function(data, nc=15, seed=1234){\n  wss &lt;- (nrow(data)-1)*sum(apply(data,2,var))\n  for (i in 2:nc){\n    set.seed(seed)\n    wss[i] &lt;- sum(kmeans(data, centers=i)$withinss)}\n  plot(1:nc, wss, type=\"b\", xlab=\"Number of Clusters\",\n       ylab=\"Within groups sum of squares\")\n}\n\n# plotting values for each cluster starting from 1 to 9\nwssplot(wine_subset, nc = 9)\n\n\n\n\n\n\n\nwine_cluster$cluster = as.factor(wine_cluster$cluster)\npairs(wine[2:4],\n      col = c(\"firebrick1\", \"darkblue\", \"forestgreen\")[wine_cluster$cluster],\n      pch = c(15:17)[wine_cluster$cluster],\n      main = \"K-Means Clusters: Wine data\")\n\n\n\n\n\n\n\ntable(wine_cluster$cluster)\n\n\n 1  2  3 \n48 60 70 \n\nlibrary(factoextra)\nfviz_nbclust(wine_subset, kmeans, method = \"wss\")\n\n\n\n\n\n\n\nwine.km &lt;- eclust(wine_subset, \"kmeans\", nboot = 2)\n\n\n\n\n\n\n\nwine.km\n\nK-means clustering with 3 clusters of sizes 60, 70, 48\n\nCluster means:\n     Alcohol      Malic        Ash\n1  0.8914655 -0.4522073  0.5406223\n2 -0.8649501 -0.5660390 -0.6371656\n3  0.1470536  1.3907328  0.2534220\n\nClustering vector:\n  [1] 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 3 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1\n [38] 1 2 3 1 3 1 3 2 3 3 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 1 2 2 1 1 1\n [75] 2 2 2 2 2 3 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n[112] 2 3 2 2 2 2 2 3 2 2 1 3 3 3 2 2 2 2 3 2 3 2 3 2 2 3 3 3 3 3 1 3 3 3 3 3 3\n[149] 3 3 3 3 1 3 2 3 3 3 1 1 3 3 3 3 1 3 3 3 1 3 2 2 1 3 3 3 1 3\n\nWithin cluster sum of squares by cluster:\n[1]  67.98619 111.63512  73.71460\n (between_SS / total_SS =  52.3 %)\n\nAvailable components:\n\n [1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n [6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"       \"clust_plot\"  \n[11] \"silinfo\"      \"nbclust\"      \"data\"         \"gap_stat\"    \n\nwine.km$nbclust\n\n[1] 3\n\nfviz_nbclust(wine_subset, kmeans, method = \"gap_stat\")\n\n\n\n\n\n\n\nfviz_silhouette(wine.km)\n\n  cluster size ave.sil.width\n1       1   60          0.44\n2       2   70          0.33\n3       3   48          0.30\n\n\n\n\n\n\n\n\nfviz_cluster(wine_cluster, data = wine_subset) + \n  theme_bw() +\n  theme(text = element_text(family=\"Georgia\")) \n\n\n\n\n\n\n\nfviz_cluster(wine_cluster, data = wine_subset, ellipse.type = \"norm\") + \n  theme_bw() +\n  theme(text = element_text(family=\"Georgia\"))"
  },
  {
    "objectID": "6323/assignment4.html#hierarchical-clustering",
    "href": "6323/assignment4.html#hierarchical-clustering",
    "title": "Knowledge Mining (EPPS 6323)",
    "section": "",
    "text": "arrest.hc &lt;- USArrests %&gt;%\n  scale() %&gt;%                    # Scale all variables\n  dist(method = \"euclidean\") %&gt;% # Euclidean distance for dissimilarity \n  hclust(method = \"ward.D2\")     # Compute hierarchical clustering\n\n# Generate dendrogram using factoextra package\nfviz_dend(arrest.hc, k = 4, # Four groups\n          cex = 0.5, \n          k_colors = c(\"firebrick1\",\"forestgreen\",\"blue\", \"purple\"),\n          color_labels_by_k = TRUE, # color labels by groups\n          rect = TRUE, # Add rectangle (cluster) around groups,\n          main = \"Cluster Dendrogram: USA Arrest data\"\n) + theme(text = element_text(family=\"Georgia\")) \n\n\n\n\n\n\n\n\nReferences\nJames, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2013 An introduction to statistical learning. Vol. 112. New York: Springer."
  },
  {
    "objectID": "6323/proposal.html",
    "href": "6323/proposal.html",
    "title": "EPPS 6323.001 Project Proposal",
    "section": "",
    "text": "Research Statement\nOur aim is to leverage knowledge-mining techniques to understand digitalization practices within government institutions. Specifically, in data-driven societies, the protection of citizen data is a growing concern. Policy interventions such as various state data breach disclosure laws (NCSL 2022) have emerged as tools to ensure transparency and better governance of personal data. Inspired by studies such as Gay’s (2017) and Sullivan’s (2010), this project will look at the public sector to ascertain the strength of correlations between select financial metrics and whether a city government was hacked or not (i.e., a binary-dependent variable). If we find statistically significant relationships via our slated approaches, we may further analyze the effects of cyberattacks on the overall financial performance of the affected entities.\n\n\nBackground\nAccording to Privacy Rights.org’s 2023 report on data breaches, there have been 20,030 data breaches in the U.S. since 2005 that have leaked out a total of 1,993,415,481 records. The main sources of breaches are direct hacks, ransomware attacks, and third-party breaches. This means an individual’s personally identifiable information (PII), such as their name, full social security number, date of birth, address, ID numbers, medical history, banking information, and other undisclosed records, are possibly compromised or stolen. Such events victimize individuals and put them at risk of falling victim to identity and/or financial fraud. Billions of compromised individual records, with an average cost of $9.44 million per breach (Morgan Lewis 2022), have posed and continue to pose a great economic burden on both the private and public sectors. Yet, there are few studies that have analyzed the characteristics of governments that are targets of cyber-attacks, a gap we hope to fill with this project.\n\n\nMethodology\nFinancial data will be derived from the Electronic Municipal Market Access (EMMA) dataset, a product of the Municipal Securities Rulemaking Board (MSRB). Breach data will be acquired from various state-wide disclosures. We plan to use unsupervised methods such as K-Means Clustering, Hierarchical Clustering, and Principal Components Analysis (Sohil et al. 2022, ch. 12-13) in order to help tease apart relationships between all our data. We will also use various supervised machine learning methods such as logistic, K-nearest neighbor, decision tree-based, and support vector machines classification methods, along with polynomial and spline regression (Sohil et al. 2022, ch. 4-9) in order to find information concerning any structured relationships between these variables. Specifically using the fact of a local government experiencing a breach or not as our dependent variable. Additionally, we plan on using re-sampling methods such as cross-validation and bootstrapping (Sohil et al. 2022, ch. 5) to test the strength of various approaches. This research will seek to contribute to policy analysis more generally by identifying patterns and correlations that elucidate the evolving role of digitalization in the context of modern governance. Of course, the results of our findings will be limited to the locales we study and not directly generalizable to other cities, states, nations, etc.\n\n\nReferences\nGay, Sebastien. 2017. “Strategic News Bundling and Privacy Breach Disclosures.” Journal of Cybersecurity 3(2): 91–108. doi:10.1093/cybsec/tyx009. Morgan Lewis. “Study Finds Average Cost of Data Breaches Reaches All-Time High in 2022.” https://www.morganlewis.com/blogs/sourcingatmorganlewis/2023/01/study-findsaverage-cost-of-data-breaches-reaches-all-time-high-in-2022 (February 20, 2024). MSRB. “Municipal Securities Rulemaking Board::EMMA.” https://emma.msrb.org/ (February 20, 2024). NCSL. 2022. “Security Breach Notification Laws.” Security Breach Notification Laws. https://www.ncsl.org/technology-and-communication/security-breach-notification-laws# (February 20, 2024). PrivacyRights.org. “United States Data Breach Notification in the United States 2023 Report | Privacy Rights Clearinghouse.” Data Breaches. https://privacyrights.org/resources/united-states-data-breach-notification-united-states2023-report (February 20, 2024). Sohil, Fariha, Muhammad Umair Sohali, and Javid Shabbir. 2022. “An Introduction to Statistical Learning with Applications in R: By Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, New York, Springer Science and Business Media, 2013, $41.98, eISBN: 978-1-4614-7137-7.” Statistical Theory and Related Fields 6(1): 87–87. doi:10.1080/24754269.2021.1980261. Sullivan, Richard J. 2010. “The Changing Nature of U.S. Card Payment Fraud: Industry and Public Policy Options.”"
  },
  {
    "objectID": "qtfiles/personalstatement.html",
    "href": "qtfiles/personalstatement.html",
    "title": "Personal Statement",
    "section": "",
    "text": "Whispering with excitement to my friend, I exclaimed, “Watch this.”\n\ninstall.packages('dplyr')\n\nMy index finger slammed onto the enter key, as if attempting to drive it through the keyboard. An irresistible grin spread across my face as the glorious stream of text flowed through the console, allowing me to manifest my inner ‘hackerman,’ culminating in the satisfying message:\n\ndownloaded 1.5 MB\n\nTurning to my friend, who had been tutoring me, I observed what I would like to consider stone-faced awe. Truth be told, entering the Social Data Analytics and Research (SDAR) master’s program with a background in biochemistry has proven to be a challenging yet rewarding transition. While I had become comfortable physically generating data and performing statistical analysis in the Palmer lab, handling these tasks virtually and exclusively through code posed a different, yet gratifying challenge.\nIn just three months into the program, I’ve already identified parallels between my data science coursework and my professional experience as a scientist. My presentation skills from participating in the Summer Platform for Undergraduate Research (SPUR) translated seamlessly, highlighting my aptitude for explaining complex work in simple terms and defending project intricacies under scrutiny. Additionally, leading the PFAS team in the Palmer lab has equipped me with invaluable experience in managing social and logistical complexities within a team setting. My skill set has prepared me to excel in my classes and collaborate effectively with my peers.\nDespite being a novice in coding, my rapid progress is evident in the creation of my personal website. I find myself improving the site as I update it with new assignments. This platform showcases my achievements and serves as a testament to my coding proficiency and continuous improvement.\nUltimately, my aim is to cultivate my skills as a data scientist at the University of Texas at Dallas and to leverage my biochemistry background to make meaningful contributions to the medical industry. I hope to utilize the scholarship funds to invest in my education, paving the way for a future where my coding prowess can measurably impact people’s lives. I aspire to be a true ‘hackerman’ and to a future where the power of my enter key can make a tangible and positive difference in the world."
  },
  {
    "objectID": "6354/assignment3.html",
    "href": "6354/assignment3.html",
    "title": "Information Management (EPPS 6354)",
    "section": "",
    "text": "1. Open the Online SQL interpreter (https://www.db-book.com/db7/university-lab- dir/sqljs.html)\n\n\n2. Write SQL codes to get a list of:\n\n\n\ni. Students IDs (hint: from the takes relation)\nSELECT DISTINCT ID FROM takes\n\n\nii. Instructors\nSELECT name FROM instructor\n\n\niii. Departments\nSELECT dept_name FROM department\n\n\n\n3. Write in SQL codes to do following queries:\n\n\n\ni. Find the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\nSELECT DISTINCT s.ID, s.name.\nFROM student s.\nJOIN takes t ON s.ID = t.ID.\nJOIN course c ON t.course_id = c.course_id.\nWHERE c.dept_name = ‘Comp. Sci.’\n\n\nii. Add grades to the list\nSELECT DISTINCT s.ID, s.name, t.grade\nFROM student s\nJOIN takes t ON s.ID = t.ID\nJOIN course c ON t.course_id = c.course_id\nWHERE c.dept_name = ‘Comp. Sci.’;\n\n\niii. Find the ID and name of each student who has not taken any course offered before 2017.\nSelect DISTINCT s.ID, s.name\nFROM student s\nJOIN takes t ON s.ID = t.ID\nWHERE year &lt; 2017\n\n\niv. For each department, find the maximum salary of instructors in that department. You may assume that every department has at least one instructor.\nSELECT dept_name, MAX(salary)\nFROM instructor\nGROUP BY dept_name;\n\n\nv. Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\nSELECT MIN(max_salary) AS min_of_max_salaries\nFROM (\nSELECT MAX(salary) AS max_salary\n\nFROM instructor\n\nGROUP BY dept_name\n) AS max_salaries;\n\n\nvi. Add names to the list\nSELECT name, MIN(max_salary) AS min_of_max_salaries\nFROM (\nSELECT name, MAX(salary) AS max_salary\n\nFROM instructor\n\nGROUP BY dept_name\n) AS max_salaries;\n\n\n4. Find instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)\nInstructor (IID)\n\n\n5. Write SQL query to find the number of students in each section. The result columns should appear in the order “courseid, secid, year, semester, num”. You do not need to output sections with 0 students."
  }
]