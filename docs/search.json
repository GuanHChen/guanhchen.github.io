[
  {
    "objectID": "epps6356/data_visualization_review.html",
    "href": "epps6356/data_visualization_review.html",
    "title": "Data Visualization",
    "section": "",
    "text": "The primary goal of data visualization is to transform data into something that people can engage with. The transformation of data from tabular format into a visual such as a trend line on a graph can condense a vast amount of information into something instantaneously comprehensible. Although the general goal of data visualization is to make data easier to consume, the priorities can change depending on the audience the data is intended for.\nFor journalism, the demographics of the audience is extremely broad and visuals must be accessible and cater to the audience members with the least background knowledge. Journalistic data visualization also tends to be more visually appealing due to the need to attract views; sometimes, this may conflict with presenting data in the most informative way. Additionally, time may be a factor in that journalistic data visuals have a tight deadline to meet. This may lead to strategic compromises given the limited resources that journalists have to work with.\nOn the other hand, academic data visualization tends to cater towards peers within the same field. Visuals do not need to be excessively aesthetically attractive since the priority above all is rigor and accuracy. In academia, researchers tend to have many opportunities (drafts, peer reviews, revisions) to refine and iterate over their work, culminating in visuals that are typically robust and informative.\nOverall, journalistic and academic data visualization share some goals and similarities, but priorities diverge due to the resources that professionals are subject to and also the intended audience for the visuals. These differences shape the ultimate design choices, detail, and objective of visuals and the way they’re utilized in storytelling."
  },
  {
    "objectID": "epps6356/data_visualization_review.html#academic-vs-journalistic-data-visualization",
    "href": "epps6356/data_visualization_review.html#academic-vs-journalistic-data-visualization",
    "title": "Data Visualization",
    "section": "",
    "text": "The primary goal of data visualization is to transform data into something that people can engage with. The transformation of data from tabular format into a visual such as a trend line on a graph can condense a vast amount of information into something instantaneously comprehensible. Although the general goal of data visualization is to make data easier to consume, the priorities can change depending on the audience the data is intended for.\nFor journalism, the demographics of the audience is extremely broad and visuals must be accessible and cater to the audience members with the least background knowledge. Journalistic data visualization also tends to be more visually appealing due to the need to attract views; sometimes, this may conflict with presenting data in the most informative way. Additionally, time may be a factor in that journalistic data visuals have a tight deadline to meet. This may lead to strategic compromises given the limited resources that journalists have to work with.\nOn the other hand, academic data visualization tends to cater towards peers within the same field. Visuals do not need to be excessively aesthetically attractive since the priority above all is rigor and accuracy. In academia, researchers tend to have many opportunities (drafts, peer reviews, revisions) to refine and iterate over their work, culminating in visuals that are typically robust and informative.\nOverall, journalistic and academic data visualization share some goals and similarities, but priorities diverge due to the resources that professionals are subject to and also the intended audience for the visuals. These differences shape the ultimate design choices, detail, and objective of visuals and the way they’re utilized in storytelling."
  },
  {
    "objectID": "epps6356/as1.html",
    "href": "epps6356/as1.html",
    "title": "Data Visualization",
    "section": "",
    "text": "## Data Visualization\n## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\n\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\n\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\n\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\n\n\n\n\npar(op)\n\nAnscombe’s regression datasets share almost identical descriptive statistics, yet appear markedly distinct from each other when visualized on a plot. Anscombe argues that relying strictly on descriptive statistics can be misleading, and that data visualization is a key component of data analysis. However, given sufficient statistical descriptions, it is possible to discriminate between datasets. That being the case, visualizations remain the most effective way of communicating differences in trends and outliers."
  },
  {
    "objectID": "epps6356/as1.html#anscombe",
    "href": "epps6356/as1.html#anscombe",
    "title": "Data Visualization",
    "section": "",
    "text": "## Data Visualization\n## Objective: Identify data or model problems using visualization\n## Anscombe (1973) Quartlet\n\ndata(anscombe)  # Load Anscombe's data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\n\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\n\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\n\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"red\", pch = 21, bg = \"orange\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"blue\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\n\n\n\n\npar(op)\n\nAnscombe’s regression datasets share almost identical descriptive statistics, yet appear markedly distinct from each other when visualized on a plot. Anscombe argues that relying strictly on descriptive statistics can be misleading, and that data visualization is a key component of data analysis. However, given sufficient statistical descriptions, it is possible to discriminate between datasets. That being the case, visualizations remain the most effective way of communicating differences in trends and outliers."
  },
  {
    "objectID": "epps6356/as1.html#fall.r",
    "href": "epps6356/as1.html#fall.r",
    "title": "Data Visualization",
    "section": "Fall.R",
    "text": "Fall.R\n\n# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n\n# install.packages(\"gsubfn\")\n# install.packages(\"tidyverse\")\nlibrary(gsubfn)\nlibrary(tidyverse)\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n\n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"darkslategray\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes"
  },
  {
    "objectID": "epps6356/as1.html#chart-review",
    "href": "epps6356/as1.html#chart-review",
    "title": "Data Visualization",
    "section": "Chart Review",
    "text": "Chart Review\nIn the paper “Canaries in the Coal Mine? Six Facts about the Recent Employment Effects of Artificial Intelligence” by Erik Brynjolfsson, Bharat Chandar, and Ruyu Chen, the researchers discuss the effect of artificial intelligence (AI) on employment by field. In the graph below, the count of software developers by age group is shown over time.\n\n\n\n\n\nLine graph showing employment trends of software engineers by age group over time. The data is normalized to 1 in October 2022.\n\n\n\nThis line chart highlights the harrowing trend of employment for entry level software engineers. However, there are a few modifications that can be done to improve the effectiveness of the graph. There are a lot of age groups clustered tightly around each other, but the main trend is that early career software developers are not getting hired. With that in mind, changing the colors for the older age groups to muted pastel colors while keeping the bright colors for the early career age groups can further endorse the story that young software engineers are struggling in the market. To further hone in on this trend, the age groups could be simplified to just early, middle, and senior career to reduce noise and clutter. Additionally, it’s not immediately clear what the lines represent until the viewer analyzes the legend on the right. Attaching labels to each line will reduce the distance that viewers’ eyes need to travel, facilitating better information communication. Finally, the x-axis appears cluttered with the year-month format. A 2 tier x-axis could be implemented with the lower level showing the year and the upper level showing the month, preserving continuity but making date information more digestible. These changes would turn an already good line graph into visual candy: a clean, compelling, and effortless way for audiences to enjoy a story."
  },
  {
    "objectID": "epps6356/Druckrey_Review.html",
    "href": "epps6356/Druckrey_Review.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Inge Druckrey: Teaching to See\nThe video explores a unique artistic paradigm: defining objects by their visual features rather than their function. For instance, a banana is perceived not as food, but as a curve with contours that convey information like the thickness of its peel. This method of breaking down objects into their visual components is a fundamental principle for an artist, enabling them to observe the world as it truly is constructed, not as they have been conditioned to see it.\nThe documentary further characterizes the roles of the eye and the hand as opposing forces in the history of writing. The eye is portrayed as a conservative force that seeks order and perfection in letterforms. Conversely, the hand is a radical force, driven by a desire for speed and expression. This inherent conflict between the eye’s pursuit of a consistent, perfect pattern and the hand’s natural haste shapes the evolution of typography and calligraphy.\nDrawing on his experience in calligraphy, Steve Jobs insisted that the Mac be the first personal computer to feature multiple typefaces and proportionally spaced fonts. Jobs believed that the aesthetic quality of typography added an intangible benefit to the user experience, an idea that proved revolutionary and was later adopted as an industry standard. This focus on aesthetic detail over simple utility exemplifies the same “artist’s eye” that deconstructs a banana into its visual features rather than its function.\nOur perception of visual weight in typography often differs from its geometric reality. While a letter may be precisely 20 pixels wide, its visual weight can appear off-balance. For example, a geometrically accurate letter like an “O” might look too thin compared to a “T” if its stroke weight is not optically adjusted. Therefore, good font design requires an artist’s eye to balance optical perception, rather than relying solely on geometric accuracy. This subtle manipulation of visual features ensures that the typography looks balanced and harmonious to the human eye, even if the underlying measurements are not uniform."
  },
  {
    "objectID": "qtfiles/personalstatement.html",
    "href": "qtfiles/personalstatement.html",
    "title": "Personal Statement",
    "section": "",
    "text": "Whispering with excitement to my friend, I exclaimed, “Watch this.”\n\ninstall.packages('dplyr')\n\nMy index finger slammed onto the enter key, as if attempting to drive it through the keyboard. An irresistible grin spread across my face as the glorious stream of text flowed through the console, allowing me to manifest my inner ‘hackerman,’ culminating in the satisfying message:\n\ndownloaded 1.5 MB\n\nTurning to my friend, who had been tutoring me, I observed what I would like to consider stone-faced awe. Truth be told, entering the Social Data Analytics and Research (SDAR) master’s program with a background in biochemistry has proven to be a challenging yet rewarding transition. While I had become comfortable physically generating data and performing statistical analysis in the Palmer lab, handling these tasks virtually and exclusively through code posed a different, yet gratifying challenge.\nIn just three months into the program, I’ve already identified parallels between my data science coursework and my professional experience as a scientist. My presentation skills from participating in the Summer Platform for Undergraduate Research (SPUR) translated seamlessly, highlighting my aptitude for explaining complex work in simple terms and defending project intricacies under scrutiny. Additionally, leading the PFAS team in the Palmer lab has equipped me with invaluable experience in managing social and logistical complexities within a team setting. My skill set has prepared me to excel in my classes and collaborate effectively with my peers.\nDespite being a novice in coding, my rapid progress is evident in the creation of my personal website. I find myself improving the site as I update it with new assignments. This platform showcases my achievements and serves as a testament to my coding proficiency and continuous improvement.\nUltimately, my aim is to cultivate my skills as a data scientist at the University of Texas at Dallas and to leverage my biochemistry background to make meaningful contributions to the medical industry. I hope to utilize the scholarship funds to invest in my education, paving the way for a future where my coding prowess can measurably impact people’s lives. I aspire to be a true ‘hackerman’ and to a future where the power of my enter key can make a tangible and positive difference in the world."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guan Chen",
    "section": "",
    "text": "Get In Touch\n\n\n\n  \n  \n  \n\n\n\n\nHello! I am a master’s student in the Social Data Analytics and Research program at the University of Texas at Dallas. I am working as a software engineer at Motor Controls Inc to develop custom software solutions.\nPlease feel free to reach out via email for any inquiries."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Guan Chen",
    "section": "",
    "text": "Get In Touch\n\n\n\n  \n  \n  \n\n\n\n\nHello! I am a master’s student in the Social Data Analytics and Research program at the University of Texas at Dallas. I am working as a software engineer at Motor Controls Inc to develop custom software solutions.\nPlease feel free to reach out via email for any inquiries."
  },
  {
    "objectID": "qtfiles/cv.html",
    "href": "qtfiles/cv.html",
    "title": "CV",
    "section": "",
    "text": "CV"
  },
  {
    "objectID": "epps6356/the_future_of_data_analysis_review.html",
    "href": "epps6356/the_future_of_data_analysis_review.html",
    "title": "Data Visualization",
    "section": "",
    "text": "The Future of Data Analysis\nIn The Future of Data Analysis talk, Edward Tufte discusses the importance of the philosophy of data analysis. Tufte stresses the goal of data analysis is to assist reasoning and to explain phenomena. Tufte proposes the idea that design choices, philosophy, and principles for data visualization should be a reflection of this mission.\nTufte illustrates the replication crisis in academia, pointing out that many discoveries in the social sciences cannot be replicated. This is in part due to the virulence of stuffing statistics in research papers with no care about the principle but all about the method. It has been a trend to include some sort of statistics to improve the robustness of any research results. Yet, many researchers do not do their due diligence in building a working background in statistics, and even worse is that many employ statistics such as t-tests in circumstances which violate the tests’ fundamental assumptions.\nTufte argues that data analysis should problem-oriented as opposed to method oriented. As a former academic, I have witnessed, first-hand, the mad scramble for employing (often bad) statistics. If data analysis principles are compromised, the outcome of data visualization loses power and meaning. Not only is this ineffective for explaining trends, but poor practices may even hinder and mislead audiences. In an age where there is more data than ever before and a library of statistical methods that exist alongside it, the problem is not that analysis needs to be conducted; the core issue is the right analysis must be used on the right data with an unyielding commitment to truth and ethics."
  },
  {
    "objectID": "epps6356/as2.html",
    "href": "epps6356/as2.html",
    "title": "Data Visualization",
    "section": "",
    "text": "### Paul Murrell's R examples (selected)\nlibrary(readxl)\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=16)  # Can you change pch? Yes\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2.5) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2.5)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for? Refers to which axis/side\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n#\n#\n#\n# HPI Data\ndf &lt;- read_excel('myHPI.xlsx')\ndf &lt;- na.omit(df)\n#\n#\n#\n#\n\n# Reset window\npar(mfrow = c(1,1))                 \npar(mar = c(5.1, 4.1, 4.1, 2.1))     \npar(las = 0, cex = 1)                \n\n# Scatterplot of HPI data: Life Expectancy vs Wellbeing\nplot(df$WB, df$LE, \n     main = \"Scatterplot of Well Being vs. Life Expectancy\", \n     xlab = \"Well Being (0-10)\",                       \n     ylab = \"Life Expectancy\",                       \n     pch = 18,                          \n     col = \"deepskyblue4\",\n     cex = 1.5,\n     bty = 'l'\n     )          \nabline(lm(df$LE ~ df$WB),\n       col = 'deepskyblue',\n       lwd = 2)\n\n\n\n\n\n\n\n# Histogram of GDP per Capita\n\nhist_density &lt;- density(df$GDP)\nhist(df$GDP,\n     breaks = 7,\n     freq = FALSE,\n     col = 'azure3',\n     main = 'Histogram of GDP per Capita',\n     xlab = 'GDP'\n     )\nlines(hist_density,\n      col = 'cyan4',\n      lwd = 2)\n\n\n\n\n\n\n\n# Barplot of GDP by Continent\n\ncontinent_labels &lt;- c(\n  \"1\" = \"Latin\\nAmerica\",\n  \"2\" = \"N. America\\n& Oceania\",\n  \"3\" = \"Western\\nEurope\",\n  \"4\" = \"Middle\\nEast\",\n  \"5\" = \"Africa\",\n  \"6\" = \"South\\nAsia\",\n  \"7\" = \"Eastern Europe &\\nCentral Asia\",\n  \"8\" = \"East Asia\"\n)\n\ndf_summary &lt;- aggregate(\n  GDP ~ Continent,\n  data = df,\n  FUN = mean\n)\n\ndf_summary &lt;- df_summary[order(df_summary$GDP, decreasing = TRUE), ]\ndf_summary$Continent &lt;- continent_labels[as.character(df_summary$Continent)]\n\npar(mar = c(9, 7, 4, 2)) \npar(mgp = c(4.5, 1, 0))   \nbarplot(\n  height = df_summary$GDP,\n  names.arg = df_summary$Continent,\n\n  # Plot Aesthetics\n  main = \"Average GDP by Continent\",\n  ylab = \"Average GDP\",\n  ylim = c(0, 60000),\n  las = 2,\n  col = \"azure3\"\n)\n\n\n\n\n\n\n\n# par(mar = c(5.1, 4.1, 4.1, 2.1))     \n# par(mgp = c(3, 1, 0))\n\n# Boxplot of GDP by Continent\n\ndf$Continent &lt;- factor(df$Continent,\n                       levels = 1:8,\n                       labels = continent_labels)\n\nboxplot(CF ~ Continent,\n        data = df,\n        main = \"Distribution of Carbon Footprint by Continent\",\n        ylab = \"Carbon Footprint\",\n        xlab = \"\",\n        las = 2,                \n        col = \"cyan3\"\n        )       \n\n# Surface of Wellbeing, GDP, and Life Expectancy\n# Load interpolation helper\nlibrary(akima)   # for interp()\n\n\n\n\n\n\n\n# Take your raw data\nx &lt;- df$WB\ny &lt;- df$GDP\nz &lt;- df$LE\n\nx_scaled &lt;- (x - min(x)) / (max(x) - min(x))\ny_scaled &lt;- (y - min(y)) / (max(y) - min(y))\n\ninterp_grid &lt;- interp(x_scaled, y_scaled, z, duplicate = \"mean\", nx = 30, ny = 30)\n\npar(mar = c(2, 2, 2, 2), lwd = 0.5)\n\n# 3D surface plot\npersp(interp_grid$x, interp_grid$y, interp_grid$z,\n      theta = 30, phi = 30, expand = 0.5,\n      col = \"lightblue\",\n      ticktype = \"simple\",\n      xlab = \"Well Being\",\n      ylab = \"GDP\",\n      zlab = \"Life Expectancy\")\n\n\n\n\n\n\n\n# Pie chart of gdp by continent\n\ntotal_gdp &lt;- sum(df$GDP, na.rm = TRUE)\ngdp_by_cont &lt;- tapply(df$GDP, df$Continent, sum, na.rm = TRUE)\ngdp_by_cont &lt;- gdp_by_cont[c(\n  \"Western\\nEurope\",\n  \"N. America\\n& Oceania\",\n  \"Latin\\nAmerica\",\n  \"East Asia\",\n  \"Eastern Europe &\\nCentral Asia\",\n  \"Africa\",\n  \"Middle\\nEast\",\n  \"South\\nAsia\"\n)]\n\npie(gdp_by_cont,\n    main = \"GDP Distribution by Continent\",\n    col = colorRampPalette(c(\"lightblue\", \"deepskyblue\"))(length(gdp_by_cont)),\n    labels = names(gdp_by_cont)\n    )"
  },
  {
    "objectID": "epps6356/as2.html#murrell01.r",
    "href": "epps6356/as2.html#murrell01.r",
    "title": "Data Visualization",
    "section": "",
    "text": "### Paul Murrell's R examples (selected)\nlibrary(readxl)\n## Start plotting from basics \n# Note the order\nplot(pressure, pch=16)  # Can you change pch? Yes\ntext(150, 600, \n     \"Pressure (mm Hg)\\nversus\\nTemperature (Celsius)\")\n\n\n\n\n\n\n\n#  Examples of standard high-level plots \n#  In each case, extra output is also added using low-level \n#  plotting functions.\n# \n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Scatterplot\n# Note the incremental additions\n\nx &lt;- c(0.5, 2, 4, 8, 12, 16)\ny1 &lt;- c(1, 1.3, 1.9, 3.4, 3.9, 4.8)\ny2 &lt;- c(4, .8, .5, .45, .4, .3)\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(x), c(0, 6))\nlines(x, y1)\nlines(x, y2)\npoints(x, y1, pch=16, cex=2.5) # Try different cex value?  \npoints(x, y2, pch=21, bg=\"white\", cex=2.5)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(0, 16, 4)) # What is the first number standing for? Refers to which axis/side\naxis(2, at=seq(0, 6, 2))\naxis(4, at=seq(0, 6, 2))\nbox(bty=\"u\")\nmtext(\"Travel Time (s)\", side=1, line=2, cex=0.8)\nmtext(\"Responses per Travel\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Responses per Second\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n# Histogram\n# Random data\nY &lt;- rnorm(50)\n# Make sure no Y exceed [-3.5, 3.5]\nY[Y &lt; -3.5 | Y &gt; 3.5] &lt;- NA # Selection/set range\nx &lt;- seq(-3.5, 3.5, .1)\ndn &lt;- dnorm(x)\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(Y, breaks=seq(-3.5, 3.5), ylim=c(0, 0.5), \n     col=\"gray80\", freq=FALSE)\nlines(x, dnorm(x), lwd=2)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Barplot\npar(mar=c(2, 3.1, 2, 2.1)) \nmidpts &lt;- barplot(VADeaths, \n                  col=gray(0.1 + seq(1, 9, 2)/11), \n                  names=rep(\"\", 4))\nmtext(sub(\" \", \"\\n\", colnames(VADeaths)),\n      at=midpts, side=1, line=0.5, cex=0.5)\ntext(rep(midpts, each=5), apply(VADeaths, 2, cumsum) - VADeaths/2,\n     VADeaths, \n     col=rep(c(\"white\", \"black\"), times=3:2), \n     cex=0.8)\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n# Boxplot\npar(mar=c(3, 4.1, 2, 0))\nboxplot(len ~ dose, data = ToothGrowth,\n        boxwex = 0.25, at = 1:3 - 0.2,\n        subset= supp == \"VC\", col=\"white\",\n        xlab=\"\",\n        ylab=\"tooth length\", ylim=c(0,35))\nmtext(\"Vitamin C dose (mg)\", side=1, line=2.5, cex=0.8)\nboxplot(len ~ dose, data = ToothGrowth, add = TRUE,\n        boxwex = 0.25, at = 1:3 + 0.2,\n        \n        subset= supp == \"OJ\")\nlegend(1.5, 9, c(\"Ascorbic acid\", \"Orange juice\"), \n       fill = c(\"white\", \"gray\"), \n       bty=\"n\")\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n# Persp\nx &lt;- seq(-10, 10, length= 30)\ny &lt;- x\nf &lt;- function(x,y) { r &lt;- sqrt(x^2+y^2); 10 * sin(r)/r }\nz &lt;- outer(x, y, f)\nz[is.na(z)] &lt;- 1\n# 0.5 to include z axis label\npar(mar=c(0, 0.5, 0, 0), lwd=0.5)\npersp(x, y, z, theta = 30, phi = 30, \n      expand = 0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.12, 0.3, 0.26, 0.16, 0.04, 0.12)\nnames(pie.sales) &lt;- c(\"Blueberry\", \"Cherry\",\n                      \"Apple\", \"Boston Cream\", \"Other\", \"Vanilla\")\npie(pie.sales, col = gray(seq(0.3,1.0,length=6))) \n\n\n\n\n\n\n\n#\n#\n#\n# HPI Data\ndf &lt;- read_excel('myHPI.xlsx')\ndf &lt;- na.omit(df)\n#\n#\n#\n#\n\n# Reset window\npar(mfrow = c(1,1))                 \npar(mar = c(5.1, 4.1, 4.1, 2.1))     \npar(las = 0, cex = 1)                \n\n# Scatterplot of HPI data: Life Expectancy vs Wellbeing\nplot(df$WB, df$LE, \n     main = \"Scatterplot of Well Being vs. Life Expectancy\", \n     xlab = \"Well Being (0-10)\",                       \n     ylab = \"Life Expectancy\",                       \n     pch = 18,                          \n     col = \"deepskyblue4\",\n     cex = 1.5,\n     bty = 'l'\n     )          \nabline(lm(df$LE ~ df$WB),\n       col = 'deepskyblue',\n       lwd = 2)\n\n\n\n\n\n\n\n# Histogram of GDP per Capita\n\nhist_density &lt;- density(df$GDP)\nhist(df$GDP,\n     breaks = 7,\n     freq = FALSE,\n     col = 'azure3',\n     main = 'Histogram of GDP per Capita',\n     xlab = 'GDP'\n     )\nlines(hist_density,\n      col = 'cyan4',\n      lwd = 2)\n\n\n\n\n\n\n\n# Barplot of GDP by Continent\n\ncontinent_labels &lt;- c(\n  \"1\" = \"Latin\\nAmerica\",\n  \"2\" = \"N. America\\n& Oceania\",\n  \"3\" = \"Western\\nEurope\",\n  \"4\" = \"Middle\\nEast\",\n  \"5\" = \"Africa\",\n  \"6\" = \"South\\nAsia\",\n  \"7\" = \"Eastern Europe &\\nCentral Asia\",\n  \"8\" = \"East Asia\"\n)\n\ndf_summary &lt;- aggregate(\n  GDP ~ Continent,\n  data = df,\n  FUN = mean\n)\n\ndf_summary &lt;- df_summary[order(df_summary$GDP, decreasing = TRUE), ]\ndf_summary$Continent &lt;- continent_labels[as.character(df_summary$Continent)]\n\npar(mar = c(9, 7, 4, 2)) \npar(mgp = c(4.5, 1, 0))   \nbarplot(\n  height = df_summary$GDP,\n  names.arg = df_summary$Continent,\n\n  # Plot Aesthetics\n  main = \"Average GDP by Continent\",\n  ylab = \"Average GDP\",\n  ylim = c(0, 60000),\n  las = 2,\n  col = \"azure3\"\n)\n\n\n\n\n\n\n\n# par(mar = c(5.1, 4.1, 4.1, 2.1))     \n# par(mgp = c(3, 1, 0))\n\n# Boxplot of GDP by Continent\n\ndf$Continent &lt;- factor(df$Continent,\n                       levels = 1:8,\n                       labels = continent_labels)\n\nboxplot(CF ~ Continent,\n        data = df,\n        main = \"Distribution of Carbon Footprint by Continent\",\n        ylab = \"Carbon Footprint\",\n        xlab = \"\",\n        las = 2,                \n        col = \"cyan3\"\n        )       \n\n# Surface of Wellbeing, GDP, and Life Expectancy\n# Load interpolation helper\nlibrary(akima)   # for interp()\n\n\n\n\n\n\n\n# Take your raw data\nx &lt;- df$WB\ny &lt;- df$GDP\nz &lt;- df$LE\n\nx_scaled &lt;- (x - min(x)) / (max(x) - min(x))\ny_scaled &lt;- (y - min(y)) / (max(y) - min(y))\n\ninterp_grid &lt;- interp(x_scaled, y_scaled, z, duplicate = \"mean\", nx = 30, ny = 30)\n\npar(mar = c(2, 2, 2, 2), lwd = 0.5)\n\n# 3D surface plot\npersp(interp_grid$x, interp_grid$y, interp_grid$z,\n      theta = 30, phi = 30, expand = 0.5,\n      col = \"lightblue\",\n      ticktype = \"simple\",\n      xlab = \"Well Being\",\n      ylab = \"GDP\",\n      zlab = \"Life Expectancy\")\n\n\n\n\n\n\n\n# Pie chart of gdp by continent\n\ntotal_gdp &lt;- sum(df$GDP, na.rm = TRUE)\ngdp_by_cont &lt;- tapply(df$GDP, df$Continent, sum, na.rm = TRUE)\ngdp_by_cont &lt;- gdp_by_cont[c(\n  \"Western\\nEurope\",\n  \"N. America\\n& Oceania\",\n  \"Latin\\nAmerica\",\n  \"East Asia\",\n  \"Eastern Europe &\\nCentral Asia\",\n  \"Africa\",\n  \"Middle\\nEast\",\n  \"South\\nAsia\"\n)]\n\npie(gdp_by_cont,\n    main = \"GDP Distribution by Continent\",\n    col = colorRampPalette(c(\"lightblue\", \"deepskyblue\"))(length(gdp_by_cont)),\n    labels = names(gdp_by_cont)\n    )"
  },
  {
    "objectID": "epps6356/as3.html",
    "href": "epps6356/as3.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Section 1\n\nlibrary(readxl)\ndf &lt;- read_excel('myHPI.xlsx') # import data\ndf &lt;- na.omit(df) # remove na's\n\nhist_density &lt;- density(df$GDP) # create density line\nhist(df$GDP, # histogram function\n     breaks = 7, # 7 sections\n     freq = FALSE, # use density instead of frequency\n     col = 'azure3', \n     main = 'Histogram of GDP per Capita',\n     xlab = 'GDP'\n     )\nlines(hist_density, # add density line on top of the histogram\n      col = 'cyan4',\n      lwd = 2)\n\n\n\n\n\n\n\n\n\n\nSection 2\n\ndata(anscombe)  # Load Anscombe's data\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\n\n\n\n\n\n\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\n\nUsing summary, it can be observed that the 4 regression lines are extremely similar to each other as they share similar slope and intercept values.\n\n# Default Plots\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\n\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\n\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\n\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n\n\n\n\n# New Plots With Dashed Lines and Triangle Plotting Characters\nplot(anscombe$x1, \n     anscombe$y1,\n     col = \"cyan4\",\n     pch = 17\n     )\nabline(coefficients(lm1), lty = 2)\n\n\n\n\n\n\n\nplot(anscombe$x2, \n     anscombe$y2,\n     col = \"cyan4\",\n     pch = 17\n     )\nabline(coefficients(lm2), lty = 2)\n\n\n\n\n\n\n\nplot(anscombe$x3, \n     anscombe$y3,\n     col = \"cyan4\",\n     pch = 17\n     )\nabline(coefficients(lm3), lty = 2)\n\n\n\n\n\n\n\nplot(anscombe$x4, \n     anscombe$y4,\n     col = \"cyan4\",\n     pch = 17\n     )\nabline(coefficients(lm4), lty = 2)"
  },
  {
    "objectID": "epps6356/data_pitfalls.html",
    "href": "epps6356/data_pitfalls.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Big Data Analytics Pitfalls\nIn an age where data is excessive"
  }
]