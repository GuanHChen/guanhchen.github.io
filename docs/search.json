[
  {
    "objectID": "6302/assignment4.html",
    "href": "6302/assignment4.html",
    "title": "Methods of Data Collection and Production",
    "section": "",
    "text": "rvest_wiki01.R\n\n## Workshop: Scraping webpages with R rvest package\n# Prerequisites: Chrome browser, Selector Gadget\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n# install.packages(\"rvest\")\nlibrary(rvest)\n\nurl &lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves'\n#Reading the HTML code from the Wiki website\nwikiforreserve &lt;- read_html(url)\nclass(wikiforreserve)\n\n## Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n## At Inspect tab, look for &lt;table class=....&gt; tag. Leave the table close\n## Right click the table and Copy --&gt; XPath, paste at html_nodes(xpath =)\n\nforeignreserve &lt;- wikiforreserve %&gt;%\n  html_nodes(xpath='//*[@id=\"mw-content-text\"]/div[1]/table[1]') %&gt;%\n  html_table()\nclass(foreignreserve) # Why the first column is not scrapped?\n\nfores = foreignreserve[[1]][,c(1, 2,3,4,5,6,7,8) ] # [[ ]] returns a single element directly, without retaining the list structure.\n\n\n# \nnames(fores) &lt;- c(\"Country\", \"Forexreswithgold\", \"Date1\", \"Change1\",\"Forexreswithoutgold\", \"Date2\",\"Change2\", \"Sources\")\ncolnames(fores)\n\nhead(fores$Country, n=10)\n\n# Sources column useful?\n\n## Clean up variables\n## What type is Date?\n\n# Convert Date1 variable\nfores$Date1 = as.Date(fores$Date1, format = \"%d %b %Y\")\nclass(fores$Date1)\n\nwrite.csv(fores, \"fores.csv\", row.names = FALSE) # use fwrite?\n\n\n\ngovtdata01.R\n\n## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Prerequisite: Download from website the list of files to be downloaded\n## Designed for background job\n\n# Start with a clean plate and lean loading to save memory\n \ngc(reset=T)\n\n# install.packages(c(\"purrr\", \"magrittr\")\nlibrary(purrr)\nlibrary(magrittr) # Alternatively, load tidyverse\n\n## Set path for reading the listing and home directory\n## For Windows, use \"c:\\\\directory\\\\subdirectory\\\\\"\n## For Mac, \"/Users/YOURNAME/path/\"\n\nsetwd(\"yourpath\")\nlibrary(rjson)\nlibrary(jsonlite)\nlibrary(data.table)\nlibrary(readr)\n\n## CSV method\ngovfiles= read.csv(file=\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_10_42.csv\", skip=2)\n\n## JSON method\n### rjson\ngf_list &lt;- rjson::fromJSON(file =\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\ngovfile2=dplyr::bind_rows(gf_list$resultSet)\n\n### jsonlite\ngf_list1 = jsonlite::read_json(\"https://github.com/datageneration/datamethods/raw/refs/heads/master/webdata/govinfo-search-results-2024-10-13T07_18_29.json\")\n\n### Extract the list\ngovfiles3 &lt;- gf_list1$resultSet\n\n### One more step\ngovfiles3 &lt;- gf_list1$resultSet |&gt; dplyr::bind_rows()\n\n\n# Preparing for bulk download of government documents\ngovfiles$id = govfiles$packageId\npdf_govfiles_url = govfiles1$pdfLink\npdf_govfiles_id &lt;- govfiles1$id\n\n# Directory to save the pdf's\nsave_dir &lt;- \"yourpath\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# Download files, potentially in parallel for speed\n# Simple timer, can use package like tictoc\n# \n\n## Try downloading one document\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\nresults &lt;- 1:1 %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\n## Try all five\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\nresults &lt;- 1:length(pdf_govfiles_url) %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\n# Print results\nprint(results)\n\n\n## Exercise: Try downloading 118th Congress Congressional Hearings in Committee on Foreign Affairs?\n\nThe scraped data is functional, but can be improved depending on the intention. The data is saved as individual pdf files which may make it cumbersome to manipulate. This could be improved by consolidating files.\nThe program can also be improved by downloading files in parallel. This would significantly cut down on scraping time and improve the effectiveness of the code."
  },
  {
    "objectID": "6302/assignment2.html",
    "href": "6302/assignment2.html",
    "title": "Methods of Data Collection and Production",
    "section": "",
    "text": "Google Trends Code\n\n# EPPS 6302: Google Trends data \n# Sample program for using gtrendsR for collecting Google Trends data\n# Documentation: vignette(\"quickstart\", package = \"gtrendsR\")\n# Website: https://cran.r-project.org/web/packages/gtrendsR/index.html\n# GitHub: https://github.com/PMassicotte/gtrendsR\n\n\n## Install package\ninstall.packages(\"gtrendsR\")\n\n## Load library and run gtrends\nlibrary(gtrendsR)\nHarrisTrumpElection = gtrends(c(\"Trump\",\"Harris\",\"election\"), time = \"all\")\n\n## Select data for plotting\nHarrisTrumpElection_interest &lt;- HarrisTrumpElection$interest_over_time\n\n## Plot data\npar(family=\"Georgia\")\nplot(HarrisTrumpElection_interest$hits, type=\"l\")\n\n## Try another dataset?\nborderimg = gtrends(\"border immigrant\", time = \"all\")\n\n## Collect data by timeframe\n\ngtrends(c(\"Harris\", \"Trump\"), time = \"now 1-H\") # last hour\ngtrends(c(\"Harris\", \"Trump\"), time = \"today 1-m\") # last 30 days\n\n## Collect data by country\n\ntg_gb &lt;- gtrends(c(\"immigrants\"), geo = c(\"GB\", \"US\"), time = \"all\") \n\n## Check country codes\n \ndata(\"countries\")\n\nUsing the gtrendsR package to collect data allows for much more personalization compared to the Google Trends website."
  },
  {
    "objectID": "qtfiles/cv.html",
    "href": "qtfiles/cv.html",
    "title": "CV",
    "section": "",
    "text": "CV"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Guan Chen",
    "section": "",
    "text": "Hello! I am currently a master’s student in the Social Data Analytics and Research program at the University of Texas at Dallas. I have a background as a scientist in bacterial genetics and membrane biology. My goal is to enhance my analytical abilities and pursue a career in data science.\nPlease feel free to look around my website and learn more about me!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Guan Chen",
    "section": "",
    "text": "Hello! I am currently a master’s student in the Social Data Analytics and Research program at the University of Texas at Dallas. I have a background as a scientist in bacterial genetics and membrane biology. My goal is to enhance my analytical abilities and pursue a career in data science.\nPlease feel free to look around my website and learn more about me!"
  },
  {
    "objectID": "6354/assignment4.html",
    "href": "6354/assignment4.html",
    "title": "Information Management (EPPS 6354)",
    "section": "",
    "text": "1. Explain the difference between a weak and a strong entity set.\nA strong entity set has a primary key and is independent. It is represented by a rectangle and relationships are illustrated by a diamond.\nA weak entity set has a partial discriminator key and is dependent on a strong entity. It is represented by a double rectangle and relationships are illustrated by a double diamond.\n\n\nDesign an E-R diagram for keeping track of the scoring statistics of your favorite sports team.\n\nYou should store the matches played, the scores in each match, the players in each match, and individual player scoring statistics for each match. Summary statistics should be modeled as derived attributes with an explanation as to how they are computed.\n\n\n\n\n3b.\nSELECT s.ID FROM student s WHERE tot_cred = 0\n\n\n3c.\nSELECT employee.ID\nFROM employee\nLEFT JOIN manages ON manages.ID = employee.ID\nWHERE manages.ID = “” OR IS NULL"
  },
  {
    "objectID": "6354/assignment6.html",
    "href": "6354/assignment6.html",
    "title": "Information Management (EPPS 6354)",
    "section": "",
    "text": "1.\nXML is more complex compared to JSON. JSON files can be parsed by JavaScript while XML requires an XML parser to obtain data.\n\n\n2.\n\ni.\nSELECT s.ID\nFROM student s\nLEFT JOIN advisor a ON s.ID = a.s_id\nWHERE a.i_id IS NOT NULL;\n\n\nii.\nSELECT i.name, i.ID\nFROM instructor i\nWHERE NOT EXISTS (\nSELECT c.course_id\n\nFROM course c\n\nWHERE c.dept_name = i.dept_name\n\nAND c.course_id NOT IN (\n\n    SELECT t.course_id\n    \n    FROM teaches t\n    \n    WHERE t.ID = i.ID\n    \n)\n)\nORDER BY i.name;"
  },
  {
    "objectID": "6354/assignment3.html",
    "href": "6354/assignment3.html",
    "title": "Information Management (EPPS 6354)",
    "section": "",
    "text": "1. Open the Online SQL interpreter (https://www.db-book.com/db7/university-lab- dir/sqljs.html)\n\n\n2. Write SQL codes to get a list of:\n\n\n\ni. Students IDs (hint: from the takes relation)\nSELECT DISTINCT ID FROM takes\n\n\nii. Instructors\nSELECT name FROM instructor\n\n\niii. Departments\nSELECT dept_name FROM department\n\n\n\n3. Write in SQL codes to do following queries:\n\n\n\ni. Find the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\nSELECT DISTINCT s.ID, s.name.\nFROM student s.\nJOIN takes t ON s.ID = t.ID.\nJOIN course c ON t.course_id = c.course_id.\nWHERE c.dept_name = ‘Comp. Sci.’\n\n\nii. Add grades to the list\nSELECT DISTINCT s.ID, s.name, t.grade\nFROM student s\nJOIN takes t ON s.ID = t.ID\nJOIN course c ON t.course_id = c.course_id\nWHERE c.dept_name = ‘Comp. Sci.’;\n\n\niii. Find the ID and name of each student who has not taken any course offered before 2017.\nSelect DISTINCT s.ID, s.name\nFROM student s\nJOIN takes t ON s.ID = t.ID\nWHERE year &lt; 2017\n\n\niv. For each department, find the maximum salary of instructors in that department. You may assume that every department has at least one instructor.\nSELECT dept_name, MAX(salary)\nFROM instructor\nGROUP BY dept_name;\n\n\nv. Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\nSELECT MIN(max_salary) AS min_of_max_salaries\nFROM (\nSELECT MAX(salary) AS max_salary\n\nFROM instructor\n\nGROUP BY dept_name\n) AS max_salaries;\n\n\nvi. Add names to the list\nSELECT name, MIN(max_salary) AS min_of_max_salaries\nFROM (\nSELECT name, MAX(salary) AS max_salary\n\nFROM instructor\n\nGROUP BY dept_name\n) AS max_salaries;\n\n\n4. Find instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)\nSELECT DISTINCT i.ID, i.name FROM instructor i LEFT JOIN teaches t ON i.ID = t.ID LEFT JOIN takes ta ON ta.course_id = t.course_id WHERE i.ID NOT IN ( SELECT DISTINCT t.ID FROM teaches t JOIN takes ta ON t.course_id = ta.course_id WHERE ta.grade = ‘A’ );\n\n\n5. Write SQL query to find the number of students in each section. The result columns should appear in the order “courseid, secid, year, semester, num”. You do not need to output sections with 0 students.\nSELECT t.course_id, t.sec_id, t.year, t.semester, COUNT(t.ID) AS num FROM takes t JOIN section s ON t.course_id = s.course_id AND t.sec_id = s.sec_id GROUP BY t.course_id, t.sec_id, t.year, t.semester;"
  },
  {
    "objectID": "6354/assignment1.html",
    "href": "6354/assignment1.html",
    "title": "Information Management (EPPS 6354)",
    "section": "",
    "text": "1: Name and describe three applications you have used that employed a database system to store and access persistent data. (e.g. airlines, online trade, banking, university system)\nThree applications the employ a database system include Reddit, Chase, and UTD Galaxy.\n\n\n2. Propose three applications in domain projects (e.g. criminology, economics, brain science, etc.) Be sure you include:\n\n\ni. Purpose\n\n\nii. Functions\n\n\niii. Simple interface design\nApplication 1: Crime Rate Predictor This application will use historical data to predict the likelihood of data in a specified geographic location. Police can then be optimally stationed around the city, in position to deter the highest likelihood of crime.\nApplication 2: Spending Predictor This application will be used to predict your future spending in a given month based on historical data, accounting for holidays and unforeseen accidents.\nApplication 3: Flu Tracker This application is aimed for use in the medical industry, keeping track of historical trends relating to the flu. This will predict the number of flu cases and allow hospitals to stock the appropriate amount of supplies such as vaccines.\n\n\n\n3. If data can be retrieved efficiently and effectively, why data mining is needed?\nData requires analysis (mining) in order to be useful. The raw data itself often does not reveal any substantial information.\n\n\n4. Why NoSQL systems emerged in the 2000s? Briefly contrast their features with traditional database systems.\nNoSQL allows developers to store unstructured data and have dynamic schemas. Conversely, traditional database systems use structured query language (SQL) and have determined schemas. NoSQL databases are non-relational while SQL databases are relational. Finally, NoSQL databases are horizontally scalable while SQL databases are vertically scalable.\n\n\n5. What are the things current database system cannot do?\nCurrent systems have scalability issues as well as query limitations.\n\n\n6. Describe at least three tables that might be used to store information in a social-network/social media system such as Twitter or Reddit.\nUser Table Stores information about specific users\nPost Table Stores information about specific post and its content\nRelationship Table Stores information about likes, dislikes, shares, etc from users"
  },
  {
    "objectID": "6354/assignment2.html",
    "href": "6354/assignment2.html",
    "title": "Information Management (EPPS 6354)",
    "section": "",
    "text": "1. What are the differences between relation schema, relation and instance? Give an example using the university database to illustrate.\nRelation schema is the overall design of the database. Relation refers to a single table in a relational database. Instance is a snapshot of the database in a moment of time.\nUsing the university database to illustrate, the overall database is the relation schema, the instructor table is a relation, and an instance of the database may be Fall 2019.\n\n\n2. Draw a schema diagram for the following bank database:\n\n\n\nBank Database\n\n\n\n\n3. Consider the above bank database. Assume that branch names (branch_name) and customer names (customer_name) uniquely identify branches and customers, but loans and accounts can be associated with more than one customer.\n\n\ni. What are the appropriate primary keys? (Underline each in diagram)\n\n\nii. Given your choice of primary keys, identify appropriate foreign keys."
  },
  {
    "objectID": "6354/assignment5.html",
    "href": "6354/assignment5.html",
    "title": "Information Management (EPPS 6354)",
    "section": "",
    "text": "1. An E-R diagram can be viewed as a graph. What do the following mean in terms of the structure of an enterprise schema?\n\na) The graph is disconnected.\nA disconnected graph indicates that an entity does not have a direct relationship with other entities. For example, there could be an audit table that tracks modifications to other tables.\n\n\nb) The graph has a cycle.\nA cycle is indicative of a recursive relationship or dependencies between entities.\n\n\n\n2. Construct an E-R diagram for a hospital with a set of patients and a set of medical doctors. Associate with each patient a log of the various tests and examinations conducted\n\n\n\n3. We can convert any weak entity set to a strong entity set by simply adding appropriate attributes. Why, then, do we have weak entity sets?\nRelationships can be complex or hierarchical. There may be some factor that makes modeling relationships with purely attributes difficult. Having weak entities may make modeling certain relationships easier.\n\n\n4.\n\na)\n\n\ni.\nSELECT e.ID, e.person_name\nFROM employee e\nJOIN works w ON w.ID = e.ID\nJOIN company c ON w.company_name = c.company_name\nWHERE e.city = c.city\n\n\nii.\nSELECT e.ID, e.person_name\nFROM employee e\nJOIN manages m ON m.ID = e.ID\nWHERE e.city = (SELECT e.city FROM employee e WHERE e.ID = m.ID)\n\n\niii.\nSELECT e.ID, e.person_name\nFROM employee e\nJOIN works w ON w.ID = e.ID\nJOIN company c on c.company_name = w.company_name\nWHERE w.salary &gt; (SELECT AVG(salary) FROM works WHERE company_name = c.company_name)\n\n\nb)\nThe query does not account for multiple sections. Therefore, courses taught in the Spring of 2017 appear multiple times if they have mor ethan one section. This can be fixed by using the DISTINCT keyword."
  },
  {
    "objectID": "6354/project.html",
    "href": "6354/project.html",
    "title": "Project Proposal",
    "section": "",
    "text": "Project Proposal\nDownload PowerPoint File"
  },
  {
    "objectID": "qtfiles/personalstatement.html",
    "href": "qtfiles/personalstatement.html",
    "title": "Personal Statement",
    "section": "",
    "text": "Whispering with excitement to my friend, I exclaimed, “Watch this.”\n\ninstall.packages('dplyr')\n\nMy index finger slammed onto the enter key, as if attempting to drive it through the keyboard. An irresistible grin spread across my face as the glorious stream of text flowed through the console, allowing me to manifest my inner ‘hackerman,’ culminating in the satisfying message:\n\ndownloaded 1.5 MB\n\nTurning to my friend, who had been tutoring me, I observed what I would like to consider stone-faced awe. Truth be told, entering the Social Data Analytics and Research (SDAR) master’s program with a background in biochemistry has proven to be a challenging yet rewarding transition. While I had become comfortable physically generating data and performing statistical analysis in the Palmer lab, handling these tasks virtually and exclusively through code posed a different, yet gratifying challenge.\nIn just three months into the program, I’ve already identified parallels between my data science coursework and my professional experience as a scientist. My presentation skills from participating in the Summer Platform for Undergraduate Research (SPUR) translated seamlessly, highlighting my aptitude for explaining complex work in simple terms and defending project intricacies under scrutiny. Additionally, leading the PFAS team in the Palmer lab has equipped me with invaluable experience in managing social and logistical complexities within a team setting. My skill set has prepared me to excel in my classes and collaborate effectively with my peers.\nDespite being a novice in coding, my rapid progress is evident in the creation of my personal website. I find myself improving the site as I update it with new assignments. This platform showcases my achievements and serves as a testament to my coding proficiency and continuous improvement.\nUltimately, my aim is to cultivate my skills as a data scientist at the University of Texas at Dallas and to leverage my biochemistry background to make meaningful contributions to the medical industry. I hope to utilize the scholarship funds to invest in my education, paving the way for a future where my coding prowess can measurably impact people’s lives. I aspire to be a true ‘hackerman’ and to a future where the power of my enter key can make a tangible and positive difference in the world."
  },
  {
    "objectID": "6302/assignment3.html",
    "href": "6302/assignment3.html",
    "title": "Methods of Data Collection and Production",
    "section": "",
    "text": "Quanteda Text Analytics 01\n\n#Quanteda01\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Twitter data about President Biden and Xi summit in Novemeber 2021\n# Do some background search/study on the event\n# \nsummit &lt;- read_csv(\"https://raw.githubusercontent.com/datageneration/datamethods/master/textanalytics/summit_11162021.csv\")\nprint(summit)\n\n# A tibble: 14,520 × 90\n   user_id status_id created_at          screen_name     text             source\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;           &lt;chr&gt;            &lt;chr&gt; \n 1 1.38e18   1.46e18 2021-11-16 20:10:23 DSJ78992721     \"Breaking News:… Twitt…\n 2 2.60e 8   1.46e18 2021-11-16 20:10:17 bradhooperarch  \"https://t.co/r… Twitt…\n 3 3.00e 9   1.46e18 2021-11-16 20:10:10 scarecrow1113   \"[Recap] Biden … Twitt…\n 4 3.00e 9   1.46e18 2021-11-15 19:24:04 scarecrow1113   \"U.S. President… Twitt…\n 5 1.36e18   1.46e18 2021-11-16 06:22:29 Internl_Leaks   \"#BREAKING Bide… Twitt…\n 6 1.36e18   1.46e18 2021-11-16 20:09:36 Internl_Leaks   \"#BREAKING Bide… Twitt…\n 7 1.05e18   1.46e18 2021-11-16 20:09:12 Lordsbondserver \"No Breakthroug… Twitt…\n 8 9.55e 8   1.46e18 2021-11-16 20:08:54 KevinCappskj    \"President Bide… Twitt…\n 9 1.26e18   1.46e18 2021-11-16 01:00:05 SayNoToSino     \"Joe Biden and … Twitt…\n10 1.26e18   1.46e18 2021-11-16 20:08:24 SayNoToSino     \"Why did Joe Bi… Twitt…\n# ℹ 14,510 more rows\n# ℹ 84 more variables: display_text_width &lt;dbl&gt;, reply_to_status_id &lt;dbl&gt;,\n#   reply_to_user_id &lt;dbl&gt;, reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;,\n#   is_retweet &lt;lgl&gt;, favorite_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;,\n#   quote_count &lt;lgl&gt;, reply_count &lt;lgl&gt;, hashtags &lt;chr&gt;, symbols &lt;chr&gt;,\n#   urls_url &lt;chr&gt;, urls_t.co &lt;chr&gt;, urls_expanded_url &lt;chr&gt;, media_url &lt;chr&gt;,\n#   media_t.co &lt;chr&gt;, media_expanded_url &lt;chr&gt;, media_type &lt;chr&gt;, …\n\nsum_twt = summit$text\ntoks = tokens(sum_twt)\nsumtwtdfm &lt;- dfm(toks)\nclass(toks)\n\n[1] \"tokens\"\n\n# Latent Semantic Analysis \n## (https://quanteda.io/reference/textmodel_lsa.html)\n\nsum_lsa &lt;- textmodel_lsa(sumtwtdfm, nd=4,  margin = c(\"both\", \"documents\", \"features\"))\nsummary(sum_lsa)\n\n                Length    Class     Mode   \nsk                      4 -none-    numeric\ndocs                58080 -none-    numeric\nfeatures            63972 -none-    numeric\nmatrix_low_rank 232218360 -none-    numeric\ndata            232218360 dgCMatrix S4     \n\nhead(sum_lsa$docs)\n\n              [,1]          [,2]          [,3]          [,4]\ntext1 8.670375e-03  9.539431e-03 -3.365261e-03  1.378640e-02\ntext2 8.662406e-06 -8.754517e-06 -6.159723e-06  1.673892e-05\ntext3 2.917454e-03  6.809891e-03  1.059921e-03 -3.180288e-03\ntext4 1.046103e-02  8.782783e-04 -4.359418e-03  4.941183e-03\ntext5 3.247147e-03  8.006068e-03  1.632191e-04 -4.657788e-03\ntext6 3.247147e-03  8.006068e-03  1.632191e-04 -4.657788e-03\n\nclass(sum_lsa)\n\n[1] \"textmodel_lsa\"\n\ntweet_dfm &lt;- tokens(sum_twt, remove_punct = TRUE) %&gt;%\n  dfm()\nhead(tweet_dfm)\n\nDocument-feature matrix of: 6 documents, 15,927 features (99.89% sparse) and 0 docvars.\n       features\ndocs    breaking news us president biden amp communist china leader xi\n  text1        1    1  1         1     1   1         1     2      1  1\n  text2        0    0  0         0     0   0         0     0      0  0\n  text3        0    0  0         0     1   0         0     0      0  1\n  text4        0    0  0         1     1   0         0     0      0  1\n  text5        0    0  0         0     1   0         0     0      0  1\n  text6        0    0  0         0     1   0         0     0      0  1\n[ reached max_nfeat ... 15,917 more features ]\n\ntag_dfm &lt;- dfm_select(tweet_dfm, pattern = \"#*\")\ntoptag &lt;- names(topfeatures(tag_dfm, 50))\nhead(toptag, 10)\n\n [1] \"#china\"       \"#biden\"       \"#xijinping\"   \"#joebiden\"    \"#america\"    \n [6] \"#americans\"   \"#coronavirus\" \"#fentanyl\"    \"#xi\"          \"#us\"         \n\nlibrary(\"quanteda.textplots\")\ntag_fcm &lt;- fcm(tag_dfm)\nhead(tag_fcm)\n\nFeature co-occurrence matrix of: 6 by 665 features.\n               features\nfeatures        #breaking #breakingnews #biden #china #usa #pray4america\n  #breaking             0             4      5      5    5             0\n  #breakingnews         0             0      4      5    4             0\n  #biden                0             0      0    443   49             0\n  #china                0             0      0      8   76             0\n  #usa                  0             0      0      0    6             0\n  #pray4america         0             0      0      0    0             0\n               features\nfeatures        #joebiden #xijinping #america #americans\n  #breaking             0          0        0          0\n  #breakingnews         0          0        0          0\n  #biden              299        370      302        295\n  #china              339        434      308        295\n  #usa                 12         15        0          0\n  #pray4america         0          0        0          0\n[ reached max_nfeat ... 655 more features ]\n\ntopgat_fcm &lt;- fcm_select(tag_fcm, pattern = toptag)\ntextplot_network(topgat_fcm, min_freq = 50, edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\nuser_dfm &lt;- dfm_select(tweet_dfm, pattern = \"@*\")\ntopuser &lt;- names(topfeatures(user_dfm, 50))\nhead(topuser, 20)\n\n [1] \"@potus\"           \"@politico\"        \"@joebiden\"        \"@jendeben\"       \n [5] \"@eneskanter\"      \"@nwadhams\"        \"@phelimkine\"      \"@nahaltoosi\"     \n [9] \"@nba\"             \"@washwizards\"     \"@pelicansnba\"     \"@capitalonearena\"\n[13] \"@kevinliptakcnn\"  \"@foxbusiness\"     \"@morningsmaria\"   \"@scmpnews\"       \n[17] \"@petermartin_pcm\" \"@nytimes\"         \"@uyghur_american\" \"@kaylatausche\"   \n\nuser_fcm &lt;- fcm(user_dfm)\nhead(user_fcm, 20)\n\nFeature co-occurrence matrix of: 20 by 711 features.\n                 features\nfeatures          @youtube @bfmtv @cnn @lauhaim @barackobama @joebiden\n  @youtube               0      0    0        0            0         0\n  @bfmtv                 0      0    1        1            1         1\n  @cnn                   0      0    0        1            1         1\n  @lauhaim               0      0    0        0            1         1\n  @barackobama           0      0    0        0            0         1\n  @joebiden              0      0    0        0            0         3\n  @kamalaharris          0      0    0        0            0         0\n  @hillaryclinton        0      0    0        0            0         0\n  @billclinton           0      0    0        0            0         0\n  @cbsnews               0      0    0        0            0         0\n                 features\nfeatures          @kamalaharris @hillaryclinton @billclinton @cbsnews\n  @youtube                    0               0            0        0\n  @bfmtv                      1               1            1        1\n  @cnn                        1               1            1        1\n  @lauhaim                    1               1            1        1\n  @barackobama                1               1            1        1\n  @joebiden                   1               1            1        1\n  @kamalaharris               0               1            1        1\n  @hillaryclinton             0               0            1        1\n  @billclinton                0               0            0        1\n  @cbsnews                    0               0            0        0\n[ reached max_feat ... 10 more features, reached max_nfeat ... 701 more features ]\n\nuser_fcm &lt;- fcm_select(user_fcm, pattern = topuser)\ntextplot_network(user_fcm, min_freq = 20, edge_color = \"firebrick\", edge_alpha = 0.8, edge_size = 1)\n\n\n\n\n\n\n\n\n\n\nQuanteda Text Analytics 02\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = c(\"quanteda.textstats\", \"quanteda.textmodels\")\n# Website: https://quanteda.io/\n\nlibrary(quanteda)\nlibrary(quanteda.textmodels)\nlibrary(quanteda.textplots)\nlibrary(readr)\nlibrary(ggplot2)\n\n# Wordcloud\n# based on US presidential inaugural address texts, and metadata (for the corpus), from 1789 to present.\ndfm_inaug &lt;- corpus_subset(data_corpus_inaugural, Year &lt;= 1826) %&gt;% \n  tokens(remove_punct = TRUE) %&gt;% \n  tokens_remove(stopwords('english')) %&gt;% \n  dfm() %&gt;%\n  dfm_trim(min_termfreq = 10, verbose = FALSE)\n\nset.seed(100)\ntextplot_wordcloud(dfm_inaug)\n\n\n\n\n\n\n\ninaug_speech = data_corpus_inaugural\n\ncorpus_subset(data_corpus_inaugural, \n              President %in% c(\"Trump\", \"Obama\", \"Bush\")) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_group(groups = President) %&gt;%\n  dfm_trim(min_termfreq = 5, verbose = FALSE) %&gt;%\n  textplot_wordcloud(comparison = TRUE)\n\n\n\n\n\n\n\ntextplot_wordcloud(dfm_inaug, min_count = 10,\n                   color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))\n\n\n\n\n\n\n\ndata_corpus_inaugural_subset &lt;- \n  corpus_subset(data_corpus_inaugural, Year &gt; 1949)\nkwic(tokens(data_corpus_inaugural_subset), pattern = \"american\") %&gt;%\n  textplot_xray()\n\n\n\n\n\n\n\ntextplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n  \n)\n\n\n\n\n\n\n\n## Why is the \"communist\" plot missing?\n\ntheme_set(theme_bw())\ng &lt;- textplot_xray(\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"american\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"people\"),\n  kwic(tokens(data_corpus_inaugural_subset), pattern = \"communist\")\n)\ng + aes(color = keyword) + \n  scale_color_manual(values = c(\"blue\", \"red\", \"green\")) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nlibrary(quanteda.textstats)\nfeatures_dfm_inaug &lt;- textstat_frequency(dfm_inaug, n = 100)\n\n# Sort by reverse frequency order\nfeatures_dfm_inaug$feature &lt;- with(features_dfm_inaug, reorder(feature, -frequency))\n\nggplot(features_dfm_inaug, aes(x = feature, y = frequency)) +\n  geom_point() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\n# Get frequency grouped by president\nfreq_grouped &lt;- textstat_frequency(dfm(tokens(data_corpus_inaugural_subset)), \n                                   groups = data_corpus_inaugural_subset$President)\n\n# Filter the term \"american\"\nfreq_american &lt;- subset(freq_grouped, freq_grouped$feature %in% \"american\")  \n\nggplot(freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  scale_y_continuous(limits = c(0, 14), breaks = c(seq(0, 14, 2))) +\n  xlab(NULL) + \n  ylab(\"Frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\ndfm_rel_freq &lt;- dfm_weight(dfm(tokens(data_corpus_inaugural_subset)), scheme = \"prop\") * 100\nhead(dfm_rel_freq)\n\nDocument-feature matrix of: 6 documents, 4,346 features (85.57% sparse) and 4 docvars.\n                 features\ndocs                      my    friends        ,    before          i\n  1953-Eisenhower 0.14582574 0.14582574 4.593511 0.1822822 0.10936930\n  1957-Eisenhower 0.20975354 0.10487677 6.345045 0.1573152 0.05243838\n  1961-Kennedy    0.19467878 0.06489293 5.451006 0.1297859 0.32446463\n  1965-Johnson    0.17543860 0.05847953 5.555556 0.2339181 0.87719298\n  1969-Nixon      0.28973510 0          5.546358 0.1241722 0.86920530\n  1973-Nixon      0.05012531 0.05012531 4.812030 0.2005013 0.60150376\n                 features\ndocs                   begin      the expression       of     those\n  1953-Eisenhower 0.03645643 6.234050 0.03645643 5.176814 0.1458257\n  1957-Eisenhower 0          5.977976 0          5.034085 0.1573152\n  1961-Kennedy    0.19467878 5.580792 0          4.218040 0.4542505\n  1965-Johnson    0          4.502924 0          3.333333 0.1754386\n  1969-Nixon      0          5.629139 0          3.890728 0.4552980\n  1973-Nixon      0          4.160401 0          3.408521 0.3007519\n[ reached max_nfeat ... 4,336 more features ]\n\nrel_freq &lt;- textstat_frequency(dfm_rel_freq, groups = dfm_rel_freq$President)\n\n# Filter the term \"american\"\nrel_freq_american &lt;- subset(rel_freq, feature %in% \"american\")  \n\nggplot(rel_freq_american, aes(x = group, y = frequency)) +\n  geom_point() + \n  scale_y_continuous(limits = c(0, 0.7), breaks = c(seq(0, 0.7, 0.1))) +\n  xlab(NULL) + \n  ylab(\"Relative frequency\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\n\n\ndfm_weight_pres &lt;- data_corpus_inaugural %&gt;%\n  corpus_subset(Year &gt; 2000) %&gt;%\n  tokens(remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  dfm() %&gt;%\n  dfm_weight(scheme = \"prop\")\n\n# Calculate relative frequency by president\nfreq_weight &lt;- textstat_frequency(dfm_weight_pres, n = 10, \n                                  groups = dfm_weight_pres$President)\n\nggplot(data = freq_weight, aes(x = nrow(freq_weight):1, y = frequency)) +\n  geom_point() +\n  facet_wrap(~ group, scales = \"free\") +\n  coord_flip() +\n  scale_x_continuous(breaks = nrow(freq_weight):1,\n                     labels = freq_weight$feature) +\n  labs(x = NULL, y = \"Relative frequency\")\n\n\n\n\n\n\n\n# Only select speeches by Obama and Trump\npres_corpus &lt;- corpus_subset(data_corpus_inaugural, \n                             President %in% c(\"Obama\", \"Trump\"))\n\n# Create a dfm grouped by president\npres_dfm &lt;- tokens(pres_corpus, remove_punct = TRUE) %&gt;%\n  tokens_remove(stopwords(\"english\")) %&gt;%\n  tokens_group(groups = President) %&gt;%\n  dfm()\n\n# Calculate keyness and determine Trump as target group\nresult_keyness &lt;- textstat_keyness(pres_dfm, target = \"Trump\")\n\n# Plot estimated word keyness\ntextplot_keyness(result_keyness) \n\n\n\n\n\n\n\n# Plot without the reference text (in this case Obama)\ntextplot_keyness(result_keyness, show_reference = FALSE)\n\n\n\n\n\n\n\nlibrary(quanteda.textmodels)\n\n# Irish budget speeches from 2010 (data from quanteda.textmodels)\n# Transform corpus to dfm\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Predict Wordscores model\nws &lt;- textmodel_wordscores(ie_dfm, y = refscores, smooth = 1)\n\n# Plot estimated word positions (highlight words and print them in red)\ntextplot_scale1d(ws,\n                 highlighted = c(\"minister\", \"have\", \"our\", \"budget\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Get predictions\npred &lt;- predict(ws, se.fit = TRUE)\n\n# Plot estimated document positions and group by \"party\" variable\ntextplot_scale1d(pred, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n# Plot estimated document positions using the LBG transformation and group by \"party\" variable\n\npred_lbg &lt;- predict(ws, se.fit = TRUE, rescaling = \"lbg\")\n\ntextplot_scale1d(pred_lbg, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n# Estimate Wordfish model\nlibrary(\"quanteda.textmodels\")\nwf &lt;- textmodel_wordfish(dfm(tokens(data_corpus_irishbudget2010)), dir = c(6, 5))\n\n# Plot estimated word positions\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Plot estimated document positions\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\n\n\n\n# Transform corpus to dfm\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Run correspondence analysis on dfm\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot estimated positions and group by party\ntextplot_scale1d(ca, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n\n\n\nQuanteda Text Analysis 03\n\n# Sample program for using quanteda for text modeling and analysis\n# Documentation: vignette(\"quickstart\", package = \"quanteda.textmodels\")\n# Website: https://quanteda.io/\n\nlibrary(\"quanteda.textmodels\")\n\n# Transform corpus to dfm\ndata(data_corpus_irishbudget2010, package = \"quanteda.textmodels\")\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Set reference scores\nrefscores &lt;- c(rep(NA, 4), 1, -1, rep(NA, 8))\n\n# Predict Wordscores model\nws &lt;- textmodel_wordscores(ie_dfm, y = refscores, smooth = 1)\n\n# Plot estimated word positions (highlight words and print them in red)\ntextplot_scale1d(ws,\n                 highlighted = c(\"minister\", \"have\", \"our\", \"budget\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Get predictions\npred &lt;- predict(ws, se.fit = TRUE)\n\n# Plot estimated document positions and group by \"party\" variable\ntextplot_scale1d(pred, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n# Plot estimated document positions using the LBG transformation and group by \"party\" variable\n\npred_lbg &lt;- predict(ws, se.fit = TRUE, rescaling = \"lbg\")\n\ntextplot_scale1d(pred_lbg, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))\n\n\n\n\n\n\n\n# Estimate Wordfish model\nlibrary(\"quanteda.textmodels\")\nwf &lt;- textmodel_wordfish(dfm(tokens(data_corpus_irishbudget2010)), dir = c(6, 5))\n\n# Plot estimated word positions\ntextplot_scale1d(wf, margin = \"features\", \n                 highlighted = c(\"government\", \"global\", \"children\", \n                                 \"bank\", \"economy\", \"the\", \"citizenship\",\n                                 \"productivity\", \"deficit\"), \n                 highlighted_color = \"red\")\n\n\n\n\n\n\n\n# Plot estimated document positions\ntextplot_scale1d(wf, groups = data_corpus_irishbudget2010$party)\n\n\n\n\n\n\n\n# Transform corpus to dfm\nie_dfm &lt;- dfm(tokens(data_corpus_irishbudget2010))\n\n# Run correspondence analysis on dfm\nca &lt;- textmodel_ca(ie_dfm)\n\n# Plot estimated positions and group by party\ntextplot_scale1d(ca, margin = \"documents\",\n                 groups = docvars(data_corpus_irishbudget2010, \"party\"))"
  },
  {
    "objectID": "6302/assignment5.html",
    "href": "6302/assignment5.html",
    "title": "Methods of Data Collection and Production",
    "section": "",
    "text": "YouTubenews01.R\n\n## Collecting Social Media data: YouTube\n\n# Required Libraries\n# Install if necessary\n# install.packages(\"tuber\")\n# install.packages(\"tidyverse\")\n# install.packages(\"lubridate\")\n# install.packages(\"stringi\")\n# install.packages(\"wordcloud\")\n# install.packages(\"gridExtra\")\n# install.packages(\"httr\")\n# install.packages(\"tm\")\n\nlibrary(tuber)\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(stringi)\nlibrary(wordcloud)\nlibrary(gridExtra)\nlibrary(httr)\nlibrary(tm)\n\n\n### Step 1: Apply for the Google YouTube API\n\n\n### 1.  Go to Google Cloud Console (https://cloud.google.com/).\n### 2.  Create a new project or select an existing one.\n### 3.  Search for and enable the YouTube Data API v3.\n### 4.  Go to Credentials &gt; Create Credentials &gt; OAuth Client ID.\n### 5.  Set up the OAuth consent screen\n### 6.  Name the App (e.g. YouTube analyzer), enter support email\n### 7.  Application type: web application\n### 8.  Name: Web client 1\n### 9.  Generate Client ID and Client Secret.\n\n### Step 2: Authenticate with YouTube API\n\n#### Use your Client ID and Client Secret to authenticate.\n\n# Replace with your actual Client ID and Client Secret\nyt_oauth(\"YourClientID\", \"YourClientSecret\", token = \"\")\n\n### Important:  when running for first time, you will be prompted to:\n### 1. add the .httr-oauth to .gitignore, select 1 to consent. \n### 2. Then it will open browser to choose your Google account.  ### 3. When prompted with safety statement (\"Google hasn’t verified this app\"), click advanced and click Go to Appname (unsafe) to verify.  \n### 4. When done, the message will show \"Authentication complete. Please close this page and return to R.\"\n### 5. Return to RStudio.  When seeing:\n###  \n### \"Waiting for authentication in browser...\n### Press Esc/Ctrl + C to abort\n### Authentication complete.\n### \n### It is ready to collect YouTube data\n\n\n### Step 3: Download YouTube Data\n\n#### Here’s an example of collecting data on the “US election 2024.”\n\n#### Search for videos related to \"US election 2024\"\nyt_uselection2024 &lt;- yt_search(term = \"US election 2024\")\n\n\n#### Display the first few rows\n\nhead(yt_uselection2024)\n\n\n### Step 4: Basic Analytics on YouTube Data\n\n#### Most Frequent Words in Video Titles\n\n# Extract titles and clean up\n\n# Extract titles and clean up\ntitles &lt;- yt_uselection2024$title\ntitles_clean &lt;- tolower(titles) %&gt;%\n  stri_replace_all_regex(\"[[:punct:]]\", \"\") %&gt;%\n  str_split(\" \") %&gt;%\n  unlist()\n\n# Create a word frequency table\nword_freq &lt;- table(titles_clean)\nword_freq_df &lt;- as.data.frame(word_freq, stringsAsFactors = FALSE)\ncolnames(word_freq_df) &lt;- c(\"word\", \"freq\")\n\n# Filter common words (stop words) and plot a word cloud\nword_freq_df &lt;- word_freq_df %&gt;% filter(!word %in% tm::stopwords(\"en\"))\nset.seed(123)\nwordcloud(words = word_freq_df$word, freq = word_freq_df$freq, max.words = 50)\n\n\n### 4.2. Plot Video Publish Dates\n\n# Format publish dates and aggregate data\n\n\nyt_sm &lt;- yt_uselection2024 %&gt;%\n  mutate(publish_date = as.Date(publishedAt)) %&gt;%\n  count(publish_date)\n\n# Plot the frequency of videos published over time\nggplot(yt_sm, aes(x = publish_date, y = n)) +\n  geom_line(color = \"blue\") +\n  labs(title = \"Videos Published Over Time\", x = \"Date\", y = \"Number of Videos\") +  \n  theme_bw()\n###  4.3. Top Channels by Video Count\n\n\n# Summarize by channel\ntop_channels &lt;- yt_uselection2024 %&gt;%\n  count(channelTitle, sort = TRUE) %&gt;%\n  top_n(10)\n\n# Plot top channels\nggplot(top_channels, aes(x = reorder(channelTitle, n), y = n)) +\n  geom_bar(stat = \"identity\", fill = \"red\") +\n  coord_flip() +\n  labs(title = \"Top Channels on 'US election 2024'\", x = \"Channel\", y = \"Number of Videos\")\n\nYes, the YouTube comments is text data that can be analyzed by quanteda. The text must be cleaned and pre-processed, but there is likely information that can be mined from Youtube comment data."
  }
]